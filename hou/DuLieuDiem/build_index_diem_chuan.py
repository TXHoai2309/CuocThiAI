import json
import os
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# ====== Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ======
# ÄÆ°á»ng dáº«n file JSON Ä‘iá»ƒm chuáº©n (Ä‘Ã£ upload)
INPUT_FILE = r"D:\airdrop\CuocThiAI\hou_crawler\crawler\data\DuLieuDiem\diem_chuan_MHN_2024_tuyensinh247.json"

# ThÆ° má»¥c lÆ°u FAISS index (Ä‘áº·t cáº¡nh file hiá»‡n táº¡i -> ./data/diem_chuan_index)
OUTPUT_INDEX_DIR = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "data", "diem_chuan_index")
)

# ====== HÃ€M TIá»†N ÃCH ======
def _safe_str(x):
    return "" if x is None else str(x)

def _normalize_method(m: str) -> str:
    """
    Chuáº©n hoÃ¡ tÃªn phÆ°Æ¡ng thá»©c Ä‘á»ƒ Ä‘á»“ng nháº¥t metadata/section.
    VÃ­ dá»¥: 'Äiá»ƒm thi THPT', 'Äiá»ƒm há»c báº¡', 'Äiá»ƒm ÄGNL HN', 'Äiá»ƒm ÄÃ¡nh giÃ¡ TÆ° duy'
    """
    if not m:
        return "khÃ¡c"
    m_lower = m.lower()
    if "thpt" in m_lower:
        return "Ä‘iá»ƒm thi thpt"
    if "há»c báº¡" in m_lower:
        return "Ä‘iá»ƒm há»c báº¡"
    if "Ä‘gnl" in m_lower:
        return "Ä‘iá»ƒm Ä‘gnl hn"
    if "tÆ° duy" in m_lower:
        return "Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ tÆ° duy"
    return m_lower

# ====== LOAD & CHUYá»‚N Äá»”I JSON -> Document ======
def load_documents(json_path: str) -> List[Document]:
    """
    Ká»³ vá»ng cáº¥u trÃºc:
    {
      "source_url": "...",
      "school": "...",
      "collected_at": "...",
      "records": [
         {
           "ten_nganh": "...",
           "to_hop_mon": ["A00","D01",...],
           "diem_chuan": 23.5,
           "ghi_chu": "...",
           "_method": "Äiá»ƒm thi THPT",
           "_year": 2024
         },
         ...
      ]
    }
    """
    with open(json_path, "r", encoding="utf-8") as f:
        payload = json.load(f)

    source_url = _safe_str(payload.get("source_url"))
    school = _safe_str(payload.get("school"))
    collected_at = _safe_str(payload.get("collected_at"))
    records = payload.get("records", [])

    docs: List[Document] = []
    for i, rec in enumerate(records):
        major = _safe_str(rec.get("ten_nganh"))
        combos = rec.get("to_hop_mon") or []
        combos_str = ", ".join([_safe_str(x) for x in combos]) if combos else ""
        score = rec.get("diem_chuan")
        note = _safe_str(rec.get("ghi_chu"))
        method_raw = _safe_str(rec.get("_method"))
        method = _normalize_method(method_raw)
        year = rec.get("_year")

        # Ná»™i dung chÃ­nh Ä‘á»ƒ truy váº¥n (page_content)
        content_lines = [
            f"TrÆ°á»ng: {school}" if school else "",
            f"NÄƒm: {year}" if year else "",
            f"NgÃ nh: {major}",
            f"Tá»• há»£p mÃ´n: {combos_str}" if combos_str else "",
            f"Äiá»ƒm chuáº©n: {score}",
            f"PhÆ°Æ¡ng thá»©c: {method_raw}" if method_raw else "",
            f"Ghi chÃº: {note}" if note else "",
            f"Nguá»“n: {source_url}" if source_url else "",
        ]
        page_content = "\n".join([line for line in content_lines if line.strip()])

        # Metadata phá»¥c vá»¥ lá»c nhanh
        metadata = {
            "school": school,
            "year": year,
            "major": major,
            "combos": combos,          # list
            "score": score,
            "method": method,          # Ä‘Ã£ chuáº©n hoÃ¡
            "method_raw": method_raw,  # giá»¯ nguyÃªn báº£n gá»‘c
            "note": note,
            "source_url": source_url,
            "collected_at": collected_at,
            # Section giÃºp phÃ¢n nhÃ³m
            "section": f"Ä‘iá»ƒm chuáº©n Â· {method}" if method else "Ä‘iá»ƒm chuáº©n",
            # id gá»£i Ã½ (duy nháº¥t theo: school-year-major-method)
            "doc_id": f"{school}|{year}|{major}|{method}".lower(),
        }

        docs.append(Document(page_content=page_content, metadata=metadata))

    return docs

# ====== MAIN PIPELINE ======
def main():
    print("ğŸ“¥ Äang táº£i vÃ  xá»­ lÃ½ dá»¯ liá»‡u Ä‘iá»ƒm chuáº©n...")
    documents = load_documents(INPUT_FILE)
    print(f"âœ… Tá»•ng sá»‘ báº£n ghi: {len(documents)}")

    # Vá»›i dá»¯ liá»‡u Ä‘iá»ƒm chuáº©n, má»—i record khÃ¡ ngáº¯n -> chunk nhá», Ã­t/khÃ´ng chá»“ng láº¥n
    print("ğŸ”ª Chia nhá» tÃ i liá»‡u (náº¿u cáº§n)...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,
        chunk_overlap=50,
        separators=["\n\n", "\n", ". ", " ", ""],
    )
    split_docs = splitter.split_documents(documents)
    print(f"ğŸ“„ Tá»•ng sá»‘ Ä‘oáº¡n sau chia: {len(split_docs)}")

    print("ğŸ§  Äang nhÃºng dá»¯ liá»‡u vá»›i HuggingFaceEmbeddings...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    vectordb = FAISS.from_documents(split_docs, embedding_model)

    # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
    os.makedirs(OUTPUT_INDEX_DIR, exist_ok=True)

    print(f"ğŸ’¾ LÆ°u FAISS index vÃ o: {OUTPUT_INDEX_DIR}")
    vectordb.save_local(OUTPUT_INDEX_DIR)

    print("ğŸ‰ HoÃ n táº¥t build index Ä‘iá»ƒm chuáº©n!")

if __name__ == "__main__":
    main()
