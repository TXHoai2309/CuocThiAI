# build_index_diem_chuan.py
import json
import os
from collections import Counter
from typing import List, Dict, Any, Iterable
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings


# ====== C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (CH·ªàNH T√ôY M√îI TR∆Ø·ªúNG) ======
# V√≠ d·ª•: INPUT_FILE = "/mnt/data/all_schools_diem_chuan_2024.json"
INPUT_FILE = r"D:\airdrop\CuocThiAI\HOU\DuLieuDiem\all_schools_diem_chuan_2024.json"

# Th∆∞ m·ª•c l∆∞u FAISS index (ƒë·∫∑t c·∫°nh file hi·ªán t·∫°i -> ./data/diem_chuan_index)
OUTPUT_INDEX_DIR = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "data", "diem_chuan_index")
)


# ====== H√ÄM TI·ªÜN √çCH ======
def _safe_str(x):
    return "" if x is None else str(x)


def _normalize_method(m: str) -> str:
    """
    Chu·∫©n ho√° t√™n ph∆∞∆°ng th·ª©c ƒë·ªÉ ƒë·ªìng nh·∫•t metadata/section.
    Gom c√°c bi·∫øn th·ªÉ 'ƒêGNL HN/HCM/SPHN', 'thi ri√™ng', 'x√©t tuy·ªÉn k·∫øt h·ª£p',...
    """
    if not m:
        return "kh√°c"
    s = m.lower()

    # C√°c nh√≥m ch√≠nh
    if "thpt" in s:
        return "ƒëi·ªÉm thi thpt"
    if "h·ªçc b·∫°" in s or "hoc ba" in s:
        return "ƒëi·ªÉm h·ªçc b·∫°"
    if "ƒëgnl" in s or "dg nl" in s or "dgnl" in s or "ƒë√°nh gi√° nƒÉng l·ª±c" in s:
        return "ƒëi·ªÉm ƒëgnl"  # gom HN/HCM/SPHN
    if "t∆∞ duy" in s or "tu duy" in s or "ƒë√°nh gi√° t∆∞ duy" in s:
        return "ƒëi·ªÉm ƒë√°nh gi√° t∆∞ duy"
    if "thi ri√™ng" in s or "k·ª≥ thi ri√™ng" in s or "ky thi rieng" in s:
        return "ƒëi·ªÉm thi ri√™ng"
    if "x√©t tuy·ªÉn k·∫øt h·ª£p" in s or "xet tuyen ket hop" in s:
        return "ƒëi·ªÉm x√©t tuy·ªÉn k·∫øt h·ª£p"

    # M·∫∑c ƒë·ªãnh gi·ªØ nguy√™n d·∫°ng th∆∞·ªùng
    return s


def _normalize_combos(raw_combos: Iterable[Any]) -> List[str]:
    """
    Chu·∫©n ho√° danh s√°ch t·ªï h·ª£p m√¥n.
    - M·ªôt s·ªë d·ªØ li·ªáu c√≥ ph·∫ßn t·ª≠ d·∫°ng "A00, A01" -> t√°ch theo d·∫•u ph·∫©y.
    - Lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a, ph·∫ßn t·ª≠ r·ªóng.
    """
    out: List[str] = []
    for c in (raw_combos or []):
        for t in _safe_str(c).split(","):
            t = t.strip()
            if t:
                out.append(t)
    # Kh·ª≠ tr√πng l·∫∑p, gi·ªØ th·ª© t·ª± xu·∫•t hi·ªán
    seen = set()
    uniq = []
    for t in out:
        if t not in seen:
            uniq.append(t)
            seen.add(t)
    return uniq


def _iter_school_blocks(root_obj: Any) -> Iterable[Dict[str, Any]]:
    """
    Chu·∫©n ho√° th√†nh iterable c√°c kh·ªëi ‚Äútr∆∞·ªùng‚Äù.
    - N·∫øu g·ªëc l√† list: duy·ªát tr·ª±c ti·∫øp.
    - N·∫øu g·ªëc l√† object: b·ªçc v√†o list ƒë·ªÉ duy·ªát.
    """
    if isinstance(root_obj, list):
        return root_obj
    return [root_obj]


# ====== LOAD & CHUY·ªÇN ƒê·ªîI JSON -> Document ======
def load_documents(json_path: str) -> List[Document]:
    """
    K·ª≥ v·ªçng c·∫•u tr√∫c hi·ªán t·∫°i:
    [
      {
        "school_code": "...",
        "school_name": "...",
        "source_url": "...",
        "collected_at": "...",
        "records": [
           {
             "ten_nganh": "...",
             "to_hop_mon": ["A00","A01", ...] ho·∫∑c ["A00, A01", ...],
             "diem_chuan": 23.5,
             "ghi_chu": "...",
             "_method": "ƒêi·ªÉm thi THPT",
             "_year": 2024
           },
           ...
        ]
      },
      ...
    ]

    (C≈©ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng c≈© d·∫°ng object ƒë∆°n.)
    """
    with open(json_path, "r", encoding="utf-8") as f:
        root = json.load(f)

    docs: List[Document] = []
    school_counter = 0
    record_counter = 0

    for block in _iter_school_blocks(root):
        school_counter += 1
        source_url = _safe_str(block.get("source_url"))
        school_name = _safe_str(block.get("school_name") or block.get("school"))
        school_code = _safe_str(block.get("school_code"))
        collected_at = _safe_str(block.get("collected_at"))
        records = block.get("records", [])

        for rec in records:
            record_counter += 1
            major = _safe_str(rec.get("ten_nganh"))
            combos = _normalize_combos(rec.get("to_hop_mon"))
            combos_str = ", ".join(combos) if combos else ""
            score = rec.get("diem_chuan")
            note = _safe_str(rec.get("ghi_chu"))
            method_raw = _safe_str(rec.get("_method"))
            method = _normalize_method(method_raw)
            year = rec.get("_year")

            # N·ªôi dung ch√≠nh ƒë·ªÉ truy v·∫•n (page_content)
            content_lines = [
                f"Tr∆∞·ªùng: {school_name or school_code}",
                f"NƒÉm: {year}" if year else "",
                f"Ng√†nh: {major}",
                f"T·ªï h·ª£p m√¥n: {combos_str}" if combos_str else "",
                f"ƒêi·ªÉm chu·∫©n: {score}",
                f"Ph∆∞∆°ng th·ª©c: {method_raw}" if method_raw else "",
                f"Ghi ch√∫: {note}" if note else "",
                f"Ngu·ªìn: {source_url}" if source_url else "",
            ]
            page_content = "\n".join([line for line in content_lines if line.strip()])

            doc_id_school = (school_code or school_name or "unknown").lower()
            metadata = {
                "school": school_name or school_code,
                "school_code": school_code,
                "year": year,
                "major": major,
                "combos": combos,          # list chu·∫©n ho√°
                "score": score,
                "method": method,          # ƒë√£ chu·∫©n ho√°
                "method_raw": method_raw,  # gi·ªØ nguy√™n b·∫£n g·ªëc
                "note": note,
                "source_url": source_url,
                "collected_at": collected_at,
                # Section gi√∫p ph√¢n nh√≥m
                "section": f"ƒëi·ªÉm chu·∫©n ¬∑ {method}" if method else "ƒëi·ªÉm chu·∫©n",
                # id g·ª£i √Ω (duy nh·∫•t theo: school-year-major-method)
                "doc_id": f"{doc_id_school}|{year}|{major}|{method}".lower(),
            }

            docs.append(Document(page_content=page_content, metadata=metadata))

    if school_counter == 0:
        print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y kh·ªëi 'tr∆∞·ªùng' n√†o trong file JSON.")
    else:
        print(f"üîé Th·ªëng k√™: {school_counter} tr∆∞·ªùng, {record_counter} b·∫£n ghi.")
    return docs


# ====== MAIN PIPELINE ======
def main():
    # 1) Ki·ªÉm tra file ƒë·∫ßu v√†o
    if not os.path.exists(INPUT_FILE):
        raise FileNotFoundError(
            f"Kh√¥ng t√¨m th·∫•y INPUT_FILE: {INPUT_FILE}\n"
            f"H√£y ch·ªânh l·∫°i bi·∫øn INPUT_FILE cho ƒë√∫ng ƒë∆∞·ªùng d·∫´n."
        )

    print("üì• ƒêang t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu ƒëi·ªÉm chu·∫©n...")
    documents = load_documents(INPUT_FILE)
    print(f"‚úÖ T·ªïng s·ªë Document t·∫°o ra: {len(documents)}")

    if not documents:
        print("‚ö†Ô∏è  Kh√¥ng c√≥ Document n√†o ƒë∆∞·ª£c t·∫°o. D·ª´ng.")
        return

    # 2) Th·ªëng k√™ nhanh (tu·ª≥ ch·ªçn)
    methods = Counter([d.metadata.get("method") for d in documents])
    years = Counter([d.metadata.get("year") for d in documents])
    print("üìä Ph√¢n b·ªë ph∆∞∆°ng th·ª©c:", dict(methods))
    print("üìÜ Ph√¢n b·ªë nƒÉm:", dict(years))

    # 3) Chia nh·ªè t√†i li·ªáu (m·ªói record th∆∞·ªùng ng·∫Øn, chunk nh·ªè & √≠t overlap)
    print("üî™ Chia nh·ªè t√†i li·ªáu (n·∫øu c·∫ßn)...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,
        chunk_overlap=50,
        separators=["\n\n", "\n", ". ", " ", ""],
    )
    split_docs = splitter.split_documents(documents)
    print(f"üìÑ T·ªïng s·ªë ƒëo·∫°n sau chia: {len(split_docs)}")

    # 4) Nh√∫ng & build FAISS
    print("üß† ƒêang nh√∫ng d·ªØ li·ªáu v·ªõi HuggingFaceEmbeddings...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    vectordb = FAISS.from_documents(split_docs, embedding_model)

    # 5) L∆∞u index
    os.makedirs(OUTPUT_INDEX_DIR, exist_ok=True)
    print(f"üíæ L∆∞u FAISS index v√†o: {OUTPUT_INDEX_DIR}")
    vectordb.save_local(OUTPUT_INDEX_DIR)

    print("üéâ Ho√†n t·∫•t build index ƒëi·ªÉm chu·∫©n!")


if __name__ == "__main__":
    main()
