#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Build FAISS index cho Chuongtrinhhoc.json theo phong cÃ¡ch:
LangChain + FAISS + HuggingFaceEmbeddings

â€¢ Nháº­n diá»‡n khÃ³a TIáº¾NG VIá»†T trong dá»¯ liá»‡u thá»±c táº¿:
  - Há»c pháº§n: 'tÃªn', 'sá»‘_tÃ­n_chá»‰' (mÃ£ cÃ³ thá»ƒ khÃ´ng cÃ³)
  - MÃ´ táº£ CTÄT/PLO: 'thÃ´ng_tin_chi_tiáº¿t' vÃ  cÃ¡c khÃ³a mÃ´ táº£ khÃ¡c (gom tá»« str/dict/list)
  - Cáº¥u trÃºc: 'khá»‘i_kiáº¿n_thá»©c' â†’ 'há»c_pháº§n'/'há»c_pháº§n_báº¯t_buá»™c'/'há»c_pháº§n_tá»±_chá»n'
â€¢ Sinh Document cho:
  - Há»c pháº§n (section = "há»c pháº§n")
  - MÃ´ táº£ chÆ°Æ¡ng trÃ¬nh / PLO (section = "mÃ´ táº£ chÆ°Æ¡ng trÃ¬nh / PLO")
"""

import argparse
import json
import os
import re
from typing import Any, Dict, List, Optional, Tuple

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings


# ================== CLI ==================
def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser("Build FAISS index cho Chuongtrinhhoc.json (LangChain)")
    p.add_argument("--input", default=r"D:\airdrop\CuocThiAI\HOU\ChuongTrinhHoc\Chuongtrinhhoc.json", help="ÄÆ°á»ng dáº«n JSON Ä‘áº§u vÃ o")
    p.add_argument("--out_dir", default="./data/ctdt_index", help="ThÆ° má»¥c lÆ°u FAISS index")
    p.add_argument("--model", default="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                   help="HuggingFace sentence-transformers model")
    p.add_argument("--chunk_size", type=int, default=800)
    p.add_argument("--chunk_overlap", type=int, default=120)
    return p.parse_args()


# ================== tiá»‡n Ã­ch chuáº©n hoÃ¡ ==================
def _safe_str(x: Any) -> str:
    return "" if x is None else str(x)


def _normalize_space(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()


def _join_nonempty(lines: List[str], sep: str = "\n") -> str:
    return sep.join([_normalize_space(l) for l in lines if _normalize_space(l)])


def _lower_keys(d: Dict[str, Any]) -> Dict[str, Any]:
    return {str(k).lower(): v for k, v in d.items()}


# ================== bá»™ khÃ³a nháº­n diá»‡n ==================
MAJOR_KEYS = {
    "ngÃ nh", "nganh", "tennganh", "tÃªn ngÃ nh", "program", "program_name",
    "chuongtrinh", "chÆ°Æ¡ng trÃ¬nh",
}
DEGREE_KEYS = {"báº­c", "bac", "degree", "trinhdo", "trÃ¬nh Ä‘á»™"}
YEAR_KEYS = {"nÄƒm", "nam", "year", "khoahoc", "khoÃ¡", "khÃ³a"}
FACULTY_KEYS = {"khoa", "khoa/viá»‡n", "viá»‡n", "faculty"}

# Há»c pháº§n trong dá»¯ liá»‡u thá»±c táº¿: cÃ³ 'tÃªn', 'sá»‘_tÃ­n_chá»‰' (mÃ£ cÃ³ thá»ƒ khÃ´ng cÃ³)
COURSE_NAME_KEYS = {
    "ten_hp", "tenhocphan", "tÃªn há»c pháº§n", "ten mon", "ten_mon", "course_name",
    "hocphan", "tÃªn", "há»c pháº§n", "mon_hoc", "mÃ´n há»c", "tenmon"
}
COURSE_CODE_KEYS = {"ma_hp", "mahocphan", "mÃ£ há»c pháº§n", "course_code", "ma_mon", "ma mon", "ma_mh", "mÃ£ mÃ´n"}
CREDIT_KEYS = {"so_tin_chi", "tin_chi", "sotinchi", "credits", "tc", "sá»‘_tÃ­n_chá»‰"}
SEMESTER_KEYS = {"hoc_ky", "hocky", "semester", "ky", "kÃ¬"}
DESC_KEYS = {"mo_ta", "mota", "description", "gioi_thieu", "giá»›i thiá»‡u", "thÃ´ng_tin_chi_tiáº¿t"}
PLO_KEYS = {"plo", "chuandaura_ctdt", "chuáº©n Ä‘áº§u ra", "chuan_dau_ra", "program_learning_outcomes"}
CLO_KEYS = {"clo", "chuandaura_hocphan", "course_learning_outcomes"}
PREREQ_KEYS = {"hoc_phan_tien_quyet", "tienquyet", "prerequisite", "Ä‘iá»u kiá»‡n tiÃªn quyáº¿t"}


def _get_first(d: Dict[str, Any], keys: set) -> Optional[Any]:
    for k, v in d.items():
        if k in keys:
            return v
    return None


def _looks_like_course(obj: Dict[str, Any]) -> bool:
    L = _lower_keys(obj)
    has_name = any(k in L for k in COURSE_NAME_KEYS)
    has_credit_or_desc_or_clo = any(k in L for k in CREDIT_KEYS | DESC_KEYS | CLO_KEYS)
    # ná»›i lá»ng: chá»‰ cáº§n 'tÃªn' + (tÃ­n chá»‰ / mÃ´ táº£ / CLO)
    return has_name and has_credit_or_desc_or_clo


def _extract_course(obj: Dict[str, Any]) -> Dict[str, Any]:
    L = _lower_keys(obj)
    course = {
        "course_name": _safe_str(_get_first(L, COURSE_NAME_KEYS)) or "",
        "course_code": _safe_str(_get_first(L, COURSE_CODE_KEYS)) or "",  # cÃ³ thá»ƒ trá»‘ng
        "credits": _get_first(L, CREDIT_KEYS),
        "semester": _get_first(L, SEMESTER_KEYS),
        "desc": _safe_str(_get_first(L, DESC_KEYS)) or "",
        "clo": _get_first(L, CLO_KEYS) or [],
        "prereq": _get_first(L, PREREQ_KEYS) or "",
    }
    # chuáº©n hoÃ¡ CLO vá» list[str]
    if isinstance(course["clo"], dict):
        course["clo"] = list(course["clo"].values())
    if isinstance(course["clo"], str):
        course["clo"] = [course["clo"]]
    return course


def _extract_context(obj_stack: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Gom bá»‘i cáº£nh tá»« cÃ¡c node cha (khoa, ngÃ nh, báº­c, nÄƒm, PLO chÆ°Æ¡ng trÃ¬nh...)."""
    ctx: Dict[str, Any] = {}
    for o in obj_stack:
        L = _lower_keys(o)
        ctx.setdefault("faculty", _safe_str(_get_first(L, FACULTY_KEYS)) or None)
        ctx.setdefault("major", _safe_str(_get_first(L, MAJOR_KEYS)) or None)
        ctx.setdefault("degree", _safe_str(_get_first(L, DEGREE_KEYS)) or None)
        ctx.setdefault("year", _get_first(L, YEAR_KEYS) or None)
        # PLO cáº¥p chÆ°Æ¡ng trÃ¬nh
        plo = _get_first(L, PLO_KEYS)
        if plo is not None and ctx.get("plo") is None:
            if isinstance(plo, dict):
                plo = list(plo.values())
            if isinstance(plo, str):
                plo = [plo]
            ctx["plo"] = plo
    return ctx


# ================== duyá»‡t JSON â†’ Documents ==================
def _walk(
    data: Any,
    stack: List[Dict[str, Any]],
    collected: List[Document],
    stats: Dict[str, int]
) -> None:
    if isinstance(data, dict):
        stack.append(data)
        L = _lower_keys(data)

        # Node há»c pháº§n
        if _looks_like_course(data):
            stats["course_like"] += 1
            ctx = _extract_context(stack)
            course = _extract_course(data)

            page_content = _join_nonempty([
                f"NgÃ nh/CTÄT: {ctx.get('major')}",
                f"Báº­c: {ctx.get('degree')}",
                f"Khoa/Viá»‡n: {ctx.get('faculty')}",
                f"NÄƒm/KhoÃ¡: {ctx.get('year')}",
                "---",
                f"Há»c pháº§n: {course['course_name']}" + (f" ({course['course_code']})" if course['course_code'] else ""),
                f"TÃ­n chá»‰: {course['credits']}",
                f"Há»c ká»³: {course['semester']}",
                f"TiÃªn quyáº¿t: {course['prereq']}",
                "MÃ´ táº£:",
                course["desc"],
                "CLO:" if course["clo"] else "",
                *[f"- {c}" for c in course["clo"]],
            ])

            metadata = {
                "section": "há»c pháº§n",
                "major": ctx.get("major"),
                "degree": ctx.get("degree"),
                "faculty": ctx.get("faculty"),
                "year": ctx.get("year"),
                "course_name": course["course_name"],
                "course_code": course["course_code"],
                "credits": course["credits"],
                "semester": course["semester"],
            }
            collected.append(Document(page_content=page_content, metadata=metadata))

        else:
            # Node mÃ´ táº£/PLO cáº¥p CTÄT â€“ gom text tá»« str/dict/list
            desc_vals: List[str] = []

            def _collect_strs(x):
                if isinstance(x, str):
                    s = _normalize_space(x)
                    if s:
                        desc_vals.append(s)
                elif isinstance(x, dict):
                    for vv in x.values():
                        _collect_strs(vv)
                elif isinstance(x, list):
                    for vv in x:
                        _collect_strs(vv)

            for k, v in L.items():
                if k in DESC_KEYS:
                    _collect_strs(v)

            plo_here = _get_first(L, PLO_KEYS)
            has_any_program_text = bool(desc_vals or plo_here is not None)

            if has_any_program_text:
                stats["program_desc_like"] += 1
                ctx = _extract_context(stack)
                plo_list = ctx.get("plo")
                if plo_here is not None:
                    plo_list = plo_here
                    if isinstance(plo_list, dict):
                        plo_list = list(plo_list.values())
                    if isinstance(plo_list, str):
                        plo_list = [plo_list]

                page_content = _join_nonempty([
                    f"NgÃ nh/CTÄT: {ctx.get('major')}",
                    f"Báº­c: {ctx.get('degree')}",
                    f"Khoa/Viá»‡n: {ctx.get('faculty')}",
                    f"NÄƒm/KhoÃ¡: {ctx.get('year')}",
                    "---",
                    "MÃ´ táº£ chÆ°Æ¡ng trÃ¬nh:",
                    _join_nonempty(desc_vals, sep="\n"),
                    "PLO:" if plo_list else "",
                    *[f"- {p}" for p in (plo_list or [])],
                ])

                metadata = {
                    "section": "mÃ´ táº£ chÆ°Æ¡ng trÃ¬nh / PLO",
                    "major": ctx.get("major"),
                    "degree": ctx.get("degree"),
                    "faculty": ctx.get("faculty"),
                    "year": ctx.get("year"),
                }
                collected.append(Document(page_content=page_content, metadata=metadata))

        # Duyá»‡t sÃ¢u
        for v in data.values():
            _walk(v, stack, collected, stats)

        stack.pop()
        return

    if isinstance(data, list):
        for item in data:
            _walk(item, stack, collected, stats)
        return
    # primitive: bá» qua


def load_documents(json_path: str) -> Tuple[List[Document], Dict[str, int]]:
    with open(json_path, "r", encoding="utf-8") as f:
        payload = json.load(f)

    docs: List[Document] = []
    stats = {"course_like": 0, "program_desc_like": 0}
    _walk(payload, stack=[], collected=docs, stats=stats)

    # Khá»­ trÃ¹ng láº·p
    seen = set()
    uniq_docs: List[Document] = []
    for d in docs:
        key = (
            d.page_content,
            d.metadata.get("section"),
            d.metadata.get("major"),
            d.metadata.get("course_code"),
        )
        if key in seen:
            continue
        seen.add(key)
        uniq_docs.append(d)

    return uniq_docs, stats


# ================== main pipeline ==================
def main():
    args = parse_args()
    input_file = args.input
    out_dir = os.path.abspath(args.out_dir)

    print("ğŸ“¥ Äang táº£i vÃ  xá»­ lÃ½ dá»¯ liá»‡u chÆ°Æ¡ng trÃ¬nh há»c...")
    documents, stats = load_documents(input_file)
    print(f"ğŸ” PhÃ¡t hiá»‡n: course-like={stats['course_like']}, program-desc-like={stats['program_desc_like']}")
    print(f"âœ… Sá»‘ document trÆ°á»›c khi chia: {len(documents)}")

    print("ğŸ”ª Chia nhá» tÃ i liá»‡u...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""],
    )
    split_docs = splitter.split_documents(documents)
    print(f"ğŸ“„ Tá»•ng sá»‘ Ä‘oáº¡n sau chia: {len(split_docs)}")

    if not split_docs:
        print("âš ï¸ KhÃ´ng cÃ³ Ä‘oáº¡n nÃ o Ä‘á»ƒ nhÃºng. Kiá»ƒm tra láº¡i cáº¥u trÃºc JSON hoáº·c bá»™ khÃ³a nháº­n diá»‡n.")
        return

    print("ğŸ§  NhÃºng dá»¯ liá»‡u (HuggingFaceEmbeddings, multilingual)...")
    embedding_model = HuggingFaceEmbeddings(model_name=args.model)
    vectordb = FAISS.from_documents(split_docs, embedding_model)

    os.makedirs(out_dir, exist_ok=True)
    print(f"ğŸ’¾ LÆ°u FAISS index vÃ o: {out_dir}")
    vectordb.save_local(out_dir)

    print("ğŸ‰ HoÃ n táº¥t build index chÆ°Æ¡ng trÃ¬nh há»c!")


if __name__ == "__main__":
    main()
