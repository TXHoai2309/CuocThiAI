import json
import re
import os
import time
from urllib.parse import urlparse

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

# ==== Cáº¥u hÃ¬nh ====
BASE_DIR = os.path.dirname(__file__)
INPUT_FILE = os.path.join(BASE_DIR, "data", "menu_contents.json")
OUTPUT_FILE = os.path.join(BASE_DIR, "data", "menu_contents_refined.json")

EXCLUDE_DOMAINS = ["jshou.edu.vn", "thuvien.hou.edu.vn", "sinhvien.hou.edu.vn"]

# ==== Selenium Chrome cáº¥u hÃ¬nh headless ====
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# ==== Tiá»‡n Ã­ch ====

def extract_date(content: str) -> str:
    """TrÃ­ch xuáº¥t ngÃ y theo Ä‘á»‹nh dáº¡ng dd/mm/yyyy"""
    match = re.search(r"\b(\d{2}/\d{2}/\d{4})\b", content)
    return match.group(1) if match else None

def is_allowed_domain(url: str) -> bool:
    """Chá»‰ cho phÃ©p domain thuá»™c hou.edu.vn vÃ  khÃ´ng náº±m trong danh sÃ¡ch loáº¡i trá»«"""
    parsed = urlparse(url)
    domain = (parsed.netloc or "").lower()
    return "hou.edu.vn" in domain and all(excl not in domain for excl in EXCLUDE_DOMAINS)

def get_full_content(url: str) -> str:
    """Truy cáº­p trang vÃ  láº¥y ná»™i dung Ä‘áº§y Ä‘á»§ (náº¿u cÃ³ nÃºt Xem thÃªm/Read more)"""
    try:
        driver.get(url)
        time.sleep(2)
        try:
            btn = driver.find_element(By.XPATH, "//a[contains(text(), 'Xem thÃªm') or contains(text(), 'Read more')]")
            btn.click()
            time.sleep(1)
        except NoSuchElementException:
            pass

        # TÃ¬m cÃ¡c vÃ¹ng ná»™i dung phá»• biáº¿n
        for by, value in [
            (By.CLASS_NAME, "single-content-full"),
            (By.TAG_NAME, "article"),
            (By.CLASS_NAME, "entry-content"),
        ]:
            try:
                content_div = driver.find_element(by, value)
                return content_div.text.strip()
            except NoSuchElementException:
                continue

        return ""
    except Exception as e:
        print(f"âŒ Lá»—i khi láº¥y ná»™i dung tá»« {url}: {e}")
        return ""

# ==== Xá»­ lÃ½ refine ====

def refine_json(input_path: str, output_path: str):
    # 1) Náº¡p danh sÃ¡ch nguá»“n (thÃ´)
    with open(input_path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    # 2) Náº¡p dá»¯ liá»‡u refined cÅ© náº¿u cÃ³, Ä‘á»ƒ chá»‰ crawl URL má»›i
    existing_by_url = {}
    if os.path.exists(output_path):
        try:
            with open(output_path, "r", encoding="utf-8") as f:
                old_refined = json.load(f)
            for doc in old_refined:
                url = (doc.get("url") or "").strip()
                if url:
                    existing_by_url[url] = doc
            print(f"ğŸ“¦ ÄÃ£ náº¡p {len(existing_by_url)} bÃ i tá»« dá»¯ liá»‡u refined cÅ©.")
        except Exception as e:
            print(f"âš ï¸ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c dá»¯ liá»‡u refined cÅ©, sáº½ ghi má»›i toÃ n bá»™: {e}")
            existing_by_url = {}

    new_refined = []
    total = len(raw_data)
    processed = 0
    added = 0

    # 3) Duyá»‡t cÃ¡c URL tá»« file thÃ´, chá»‰ lÃ m vá»›i URL chÆ°a cÃ³ trong refined
    for i, item in enumerate(raw_data, start=1):
        url = (item.get("url") or "").strip()
        if not url or not is_allowed_domain(url):
            continue

        processed += 1
        if url in existing_by_url:
            print(f"â­ï¸ ({i}/{total}) Bá» qua (Ä‘Ã£ cÃ³ trong refined): {url}")
            continue

        print(f"ğŸ” ({i}/{total}) Xá»­ lÃ½ má»›i: {url}")
        content = get_full_content(url)
        if not content.strip():
            print(f"âš ï¸ Ná»™i dung rá»—ng táº¡i {url}, bá» qua.")
            continue

        # Láº¥y ngÃ y: Æ°u tiÃªn 'date' cÃ³ sáºµn, sau Ä‘Ã³ trÃ­ch tá»« content
        date_val = item.get("date") or extract_date(content)

        # Láº¥y category: Æ°u tiÃªn 'category', fallback tá»« 'category_path'
        category_val = item.get("category")
        if not category_val:
            cp = item.get("category_path")
            if isinstance(cp, list):
                category_val = " > ".join(cp)
            else:
                category_val = ""

        new_refined.append({
            "title": item.get("title", ""),
            "url": url,
            "category": category_val,
            "date": date_val,
            "content": content
        })
        added += 1

    # 4) Há»£p nháº¥t: dá»¯ liá»‡u cÅ© + cÃ¡c báº£n má»›i (vÃ¬ chá»‰ crawl URL má»›i nÃªn khÃ´ng Ä‘Ã¨ báº£n cÅ©)
    merged_by_url = dict(existing_by_url)
    for doc in new_refined:
        merged_by_url[doc["url"]] = doc  # thÃªm báº£n má»›i

    merged_list = list(merged_by_url.values())

    # 5) Ghi Ä‘Ã¨ file refined
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(merged_list, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ÄÃ£ lÆ°u refined: {output_path}")
    print(f"â€¢ Nguá»“n tá»•ng: {total}")
    print(f"â€¢ Bá» qua (Ä‘Ã£ cÃ³): {len(existing_by_url)}")
    print(f"â€¢ Má»›i thÃªm: {added}")
    print(f"â€¢ Tá»•ng sau há»£p nháº¥t: {len(merged_list)}")

# ==== Cháº¡y chÃ­nh ====
if __name__ == "__main__":
    try:
        refine_json(INPUT_FILE, OUTPUT_FILE)
    finally:
        driver.quit()
