import json
import os
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# ƒê∆∞·ªùng d·∫´n file JSON ƒë·∫ßu v√†o
INPUT_FILE =  r"D:\bai_tap_lon_cac_mon\CuocThiAI\hou_crawler\crawler\data\hou_index"
OUTPUT_INDEX_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), 'hou_crawler', 'crawler', 'data', 'hou_index'))

# T·∫£i v√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh list Document
def load_documents(json_path):
    documents = []
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    for item in data:
        content = item.get("content", "").strip()
        if not content:
            continue

        title = (item.get("title") or "").lower()
        url = (item.get("url") or "").lower()
        category = (item.get("category") or "").lower()

        # G√°n section theo logic ƒë∆°n gi·∫£n
        if any(x in title for x in ["gi·ªõi thi·ªáu", "l·ªãch s·ª≠", "s·ª© m·ªánh", "t·∫ßm nh√¨n", "tri·∫øt l√Ω"]):
            section = "gi·ªõi thi·ªáu"
        elif "tuy·ªÉn sinh" in title or "tuy·ªÉn sinh" in url:
            section = "tuy·ªÉn sinh"
        elif any(x in title or x in category for x in ["ng√†nh", "chuy√™n ng√†nh", "ch∆∞∆°ng tr√¨nh"]):
            section = "ng√†nh h·ªçc"
        elif "s·ª± ki·ªán" in category or "event" in url:
            section = "s·ª± ki·ªán"
        elif "h·ªçc ph√≠" in category or "h·ªçc ph√≠" in title:
            section = "h·ªçc ph√≠"
        elif "h·ª£p t√°c" in category or "h·ª£p t√°c" in title:
            section = "h·ª£p t√°c"
        elif "th√¥ng b√°o" in category or "th√¥ng b√°o" in title:
            section = "th√¥ng b√°o"
        else:
            section = "kh√°c"

        doc = Document(
            page_content=content,
            metadata={
                "title": item.get("title", ""),
                "url": item.get("url", ""),
                "category": item.get("category", ""),
                "section": section
            }
        )
        documents.append(doc)

    return documents

def main():
    print("üì• ƒêang t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu JSON...")
    documents = load_documents(INPUT_FILE)

    print("üî™ Chia nh·ªè t√†i li·ªáu...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    split_docs = splitter.split_documents(documents)

    print(f"üìÑ T·ªïng s·ªë ƒëo·∫°n sau chia: {len(split_docs)}")
    
    print("üß† ƒêang nh√∫ng d·ªØ li·ªáu v·ªõi HuggingFaceEmbeddings...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectordb = FAISS.from_documents(split_docs, embedding_model)

    print(f"üíæ L∆∞u FAISS index v√†o th∆∞ m·ª•c: {OUTPUT_INDEX_DIR}")
    vectordb.save_local(OUTPUT_INDEX_DIR)
    print("‚úÖ Ho√†n t·∫•t!")

if __name__ == "__main__":
    main()
