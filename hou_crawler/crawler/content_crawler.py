import json
import requests
from bs4 import BeautifulSoup
import os

INPUT_PATH = "../data/menu_links.json"
OUTPUT_PATH = "../data/menu_contents.json"

def clean_html_text(html: str) -> str:
    """L√†m s·∫°ch HTML, lo·∫°i b·ªè script/style v√† tr·∫£ v·ªÅ text thu·∫ßn"""
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "iframe"]):
        tag.decompose()
    return soup.get_text(separator="\n", strip=True)

def is_valid_url(url: str) -> bool:
    """Ch·ªâ crawl c√°c link thu·ªôc domain hou.edu.vn"""
    return url.startswith("http") and "hou.edu.vn" in url

def extract_article_links_from_category(soup):
    """L·∫•y danh s√°ch URL b√†i vi·∫øt con trong trang category"""
    links = []
    for a in soup.select("article a"):
        href = a.get("href")
        if href and is_valid_url(href):
            links.append(href)
    return list(set(links))  # lo·∫°i b·ªè tr√πng

def crawl_single_article(url, category):
    """Crawl 1 b√†i vi·∫øt chi ti·∫øt"""
    print(f"üåê ƒêang crawl b√†i: {url}")
    try:
        resp = requests.get(url, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')

        title_tag = soup.find('h1') or soup.find('title')
        title = title_tag.get_text(strip=True) if title_tag else "Kh√¥ng r√µ ti√™u ƒë·ªÅ"

        date_tag = soup.find('time') or soup.find("div", class_="post-date")
        date = date_tag.get_text(strip=True) if date_tag else None

        article = (
            soup.find("div", class_="entry-content") or
            soup.find("div", class_="elementor-post__text") or
            soup.find("div", class_="elementor-widget-container") or
            soup.find("div", class_="post-content") or
            soup.find("article") or
            soup.find("body")
        )
        if not article:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung ph√π h·ª£p ·ªü {url}")
            return None

        content_html = str(article)
        content_cleaned = clean_html_text(content_html)

        if not content_cleaned.strip():
            print(f"‚ö†Ô∏è N·ªôi dung r·ªóng t·∫°i {url}, b·ªè qua.")
            return None

        return {
            "title": title,
            "category": category,
            "url": url,
            "content_html": content_html,
            "content_cleaned": content_cleaned,
            "date": date
        }

    except Exception as e:
        print(f"‚ùå L·ªói khi crawl {url}: {e}")
        return None

def crawl_content_from_links(menu_links_path, output_path):
    with open(menu_links_path, 'r', encoding='utf-8') as f:
        links_data = json.load(f)

    results = []

    for item in links_data:
        url = item['url']
        category = item['category']

        if not is_valid_url(url):
            print(f"‚è© B·ªè qua URL kh√¥ng h·ª£p l·ªá: {url}")
            continue

        try:
            print(f"üîó ƒêang x·ª≠ l√Ω URL: {url}")
            resp = requests.get(url, timeout=10)
            soup = BeautifulSoup(resp.text, 'html.parser')

            # N·∫øu l√† trang ch·ª©a nhi·ªÅu b√†i vi·∫øt (category)
            if soup.find_all("article"):
                article_links = extract_article_links_from_category(soup)
                print(f"üìå Ph√°t hi·ªán {len(article_links)} b√†i vi·∫øt trong category {url}")
                for link in article_links:
                    article_data = crawl_single_article(link, category)
                    if article_data:
                        results.append(article_data)
            else:
                # Crawl nh∆∞ b√†i vi·∫øt ƒë∆°n
                article_data = crawl_single_article(url, category)
                if article_data:
                    results.append(article_data)

        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω {url}: {e}")

            # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
    os.makedirs(os.path.dirname(output_path), exist_ok=True) 

    # Ghi file
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\n‚úÖ ƒê√£ l∆∞u {len(results)} b√†i vi·∫øt v√†o: {output_path}")



if __name__ == "__main__":
    crawl_content_from_links(INPUT_PATH, OUTPUT_PATH)
