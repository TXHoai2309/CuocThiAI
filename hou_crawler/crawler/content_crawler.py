import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import os

visited = set()
articles_new = []      # b√†i m·ªõi trong l·∫ßn ch·∫°y
existing_urls = set()  # URL ƒë√£ c√≥ trong file k·∫øt qu·∫£

# ===== Logger c√≥ ƒë√°nh s·ªë th·ª© t·ª± d√≤ng =====
LOG_IDX = 1
def log(msg: str):
    """In ra console v·ªõi s·ªë th·ª© t·ª± 1., 2., ... tr∆∞·ªõc m·ªói d√≤ng."""
    global LOG_IDX
    print(f"{LOG_IDX}. {msg}")
    LOG_IDX += 1

def contains_hou(url: str) -> bool:
    """Ch·∫•p nh·∫≠n m·ªçi URL mi·ªÖn l√† c√≥ ch·ª©a 'hou' (kh√¥ng ph√¢n bi·ªát hoa/th∆∞·ªùng)."""
    try:
        return "hou" in url.lower()
    except:
        return False

def is_crawlable(url: str) -> bool:
    """Lo·∫°i b·ªè scheme kh√¥ng h·ª£p l·ªá."""
    if not url:
        return False
    low = url.lower()
    if low.startswith(("mailto:", "javascript:", "#")):
        return False
    return True

def clean_html_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "iframe"]):
        tag.decompose()
    return soup.get_text(separator="\n", strip=True)

def number_lines(text: str) -> str:
    """ƒê√°nh s·ªë 1., 2., ... tr∆∞·ªõc m·ªói d√≤ng kh√¥ng r·ªóng; gi·ªØ d√≤ng tr·ªëng n·∫øu c√≥."""
    lines = text.splitlines()
    out, idx = [], 1
    for line in lines:
        if line.strip():
            out.append(f"{idx}. {line}")
            idx += 1
        else:
            out.append("")
    return "\n".join(out)

def crawl_article(url, category_path, depth=0):
    indent = "  " * depth

    if url in visited:
        log(f"{indent}üîÅ B·ªè qua (ƒë√£ thƒÉm): {url}")
        return

    # B·ªè qua n·∫øu URL ƒë√£ c√≥ trong d·ªØ li·ªáu tr∆∞·ªõc ƒë√≥
    if url in existing_urls:
        visited.add(url)
        log(f"{indent}‚è≠Ô∏è B·ªè qua (ƒë√£ c√≥ trong d·ªØ li·ªáu): {url}")
        return

    visited.add(url)
    try:
        log(f"{indent}üìù B·∫Øt ƒë·∫ßu crawl b√†i: {url}")
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        title_tag = soup.find("h1") or soup.find("title")
        title = title_tag.get_text(strip=True) if title_tag else "Kh√¥ng r√µ ti√™u ƒë·ªÅ"

        date_tag = soup.find("time") or soup.find("div", class_="post-date")
        date = date_tag.get_text(strip=True) if date_tag else None

        content_div = (
            soup.find("div", class_="entry-content") or
            soup.find("div", class_="post-content") or
            soup.find("article") or
            soup.find("body")
        )
        if not content_div:
            log(f"{indent}‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung: {url}")
            return

        content_html = str(content_div)
        content_cleaned = clean_html_text(content_html)
        if not content_cleaned.strip():
            log(f"{indent}‚ö†Ô∏è N·ªôi dung r·ªóng, b·ªè qua: {url}")
            return

        # ƒê√°nh s·ªë th·ª© t·ª± tr∆∞·ªõc m·ªói d√≤ng n·ªôi dung
        content_numbered = number_lines(content_cleaned)

        article_data = {
            "title": title,
            "url": url,
            "date": date,
            "category_path": category_path,
            "content_html": content_html,
            "content_cleaned": content_numbered
        }

        articles_new.append(article_data)
        existing_urls.add(url)  # tr√°nh tr√πng trong c√πng phi√™n
        log(f"{indent}‚úÖ ƒê√£ crawl xong: {title} | {url}")

    except Exception as e:
        log(f"{indent}‚ùå L·ªói khi crawl b√†i: {url} | {e}")

def crawl_tree(url, category_name, parent_path, depth=0):
    indent = "  " * depth

    if url in visited:
        log(f"{indent}üîÅ B·ªè qua node (ƒë√£ thƒÉm): {url}")
        return
    if not is_crawlable(url):
        log(f"{indent}‚õî B·ªè qua (kh√¥ng crawlable): {url}")
        return
    if not contains_hou(url):
        log(f"{indent}‚õî B·ªè qua (kh√¥ng ch·ª©a 'hou'): {url}")
        return

    visited.add(url)
    log(f"{indent}üìÇ Duy·ªát menu: {url}")

    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        links = soup.find_all("a", href=True)

        for a in links:
            href_raw = a['href'].strip()
            if not is_crawlable(href_raw):
                log(f"{indent}  ‚õî B·ªè qua link kh√¥ng h·ª£p l·ªá: {href_raw}")
                continue

            href = urljoin(resp.url, href_raw)  # chu·∫©n h√≥a absolute URL

            if not contains_hou(href):
                log(f"{indent}  ‚õî B·ªè qua (kh√¥ng ch·ª©a 'hou'): {href}")
                continue
            if href in visited:
                log(f"{indent}  üîÅ B·ªè qua (ƒë√£ thƒÉm): {href}")
                continue

            title = a.get_text(strip=True)

            # Heuristic ph√¢n bi·ªát b√†i vi·∫øt / menu
            if "category" not in href and "page" not in href and len(href) > 20:
                log(f"{indent}  ‚ûú Ph√°t hi·ªán b√†i vi·∫øt: {href}")
                crawl_article(href, parent_path + [category_name], depth + 1)
            else:
                log(f"{indent}  ‚ûú ƒêi s√¢u menu: {href}")
                crawl_tree(href, title or category_name, parent_path + [category_name], depth + 1)

    except Exception as e:
        log(f"{indent}‚ùå L·ªói khi duy·ªát menu: {url} | {e}")

def main():
    INPUT = "../data/menu_links.json"
    OUTPUT_ARTICLES = "../data/menu_content.json"  # theo y√™u c·∫ßu

    # T·∫£i d·ªØ li·ªáu ƒë√£ c√≥ (n·∫øu t·ªìn t·∫°i) ƒë·ªÉ kh√¥ng crawl l·∫°i
    existing_articles = []
    if os.path.exists(OUTPUT_ARTICLES):
        try:
            with open(OUTPUT_ARTICLES, 'r', encoding='utf-8') as f:
                existing_articles = json.load(f)
            for it in existing_articles:
                if isinstance(it, dict) and "url" in it:
                    existing_urls.add(it["url"])
            log(f"üì¶ N·∫°p d·ªØ li·ªáu c≈©: {len(existing_articles)} b√†i.")
        except Exception as e:
            log(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c d·ªØ li·ªáu c≈©, s·∫Ω t·∫°o l·∫°i: {e}")
            existing_articles = []
            existing_urls.clear()

    # N·∫°p menu link g·ªëc
    with open(INPUT, 'r', encoding='utf-8') as f:
        menu_links = json.load(f)

    for item in menu_links:
        root_url = item.get("url", "").strip()
        root_name = item.get("category", "").strip() or "ROOT"
        if not is_crawlable(root_url):
            log(f"‚õî B·ªè qua root kh√¥ng crawlable: {root_url}")
            continue
        if not contains_hou(root_url):
            log(f"‚õî B·ªè qua root (kh√¥ng ch·ª©a 'hou'): {root_url}")
            continue
        log(f"\nüåê B·∫Øt ƒë·∫ßu t·ª´: {root_name} | {root_url}")
        crawl_tree(root_url, root_name, [], depth=0)

    # H·ª£p nh·∫•t d·ªØ li·ªáu c≈© + m·ªõi (ghi ƒë√® URL tr√πng b·∫±ng b·∫£n m·ªõi)
    merged_by_url = {}
    for it in existing_articles:
        if isinstance(it, dict) and "url" in it:
            merged_by_url[it["url"]] = it
    for it in articles_new:
        merged_by_url[it["url"]] = it  # ghi ƒë√® ƒë·ªÉ c·∫≠p nh·∫≠t n·ªôi dung m·ªõi

    merged_list = list(merged_by_url.values())

    os.makedirs(os.path.dirname(OUTPUT_ARTICLES), exist_ok=True)
    with open(OUTPUT_ARTICLES, 'w', encoding='utf-8') as f:
        json.dump(merged_list, f, ensure_ascii=False, indent=2)

    log(f"\n‚úÖ ƒê√£ l∆∞u t·ªïng c·ªông {len(merged_list)} b√†i vi·∫øt t·∫°i: {OUTPUT_ARTICLES}")
    log(f"‚ûï M·ªõi th√™m trong l·∫ßn ch·∫°y n√†y: {len(articles_new)}")

if __name__ == "__main__":
    main()
