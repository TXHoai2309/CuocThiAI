import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin, urldefrag, urlunparse
import os

# ================== Tr·∫°ng th√°i to√†n c·ª•c ==================
visited = set()         # ch·ª©a URL ƒë√£ chu·∫©n ho√° (canonical)
articles_new = []       # b√†i m·ªõi trong l·∫ßn ch·∫°y
existing_urls = set()   # URL ƒë√£ c√≥ trong file k·∫øt qu·∫£ (canonical)

# ===== Logger c√≥ ƒë√°nh s·ªë th·ª© t·ª± d√≤ng =====
LOG_IDX = 1

def log(msg: str):
    """In ra console v·ªõi s·ªë th·ª© t·ª± 1., 2., ... tr∆∞·ªõc m·ªói d√≤ng."""
    global LOG_IDX
    print(f"{LOG_IDX}. {msg}")
    LOG_IDX += 1

# ================== Ti·ªán √≠ch URL & b·ªô l·ªçc ==================

def canon(url: str, keep_query: bool = True) -> str:
    """
    Chu·∫©n ho√° URL ƒë·ªÉ so tr√πng.
    - B·ªè #fragment
    - GI·ªÆ query c√≥ √Ω nghƒ©a (page, id, ...) nh∆∞ng lo·∫°i tracking: utm_*, fbclid, gclid, msclkid, spm, ref, referrer, source
    - H·∫° ch·ªØ th∆∞·ªùng scheme/host
    - B·ªè 'www.' n·∫øu c√≥
    - B·ªè '/' cu·ªëi n·∫øu kh√¥ng ph·∫£i root
    """
    if not url:
        return ""
    try:
        u, _ = urldefrag(url.strip())
        p = urlparse(u)
        scheme = (p.scheme or "http").lower()
        netloc = (p.netloc or "").lower()
        if netloc.startswith("www."):
            netloc = netloc[4:]
        path = p.path or "/"
        if path.endswith("/") and path != "/":
            path = path[:-1]

        query = ""
        if keep_query and p.query:
            from urllib.parse import parse_qsl, urlencode
            tracking_prefixes = ("utm_",)
            tracking_names = {"fbclid", "gclid", "msclkid", "spm", "ref", "referrer", "source"}
            pairs = []
            for k, v in parse_qsl(p.query, keep_blank_values=True):
                lk = (k or "").lower()
                if lk in tracking_names:
                    continue
                if any(lk.startswith(pref) for pref in tracking_prefixes):
                    continue
                pairs.append((k, v))
            if pairs:
                query = urlencode(pairs, doseq=True)

        return urlunparse((scheme, netloc, path, "", query, ""))
    except Exception:
        return url.strip()


def normalize_url(href: str, base_url: str) -> str:
    """Tuy·ªát ƒë·ªëi ho√° & tr·∫£ v·ªÅ b·∫£n canonical c·ªßa URL (gi·ªØ query h·ªØu √≠ch)."""
    abs_url = urljoin(base_url, (href or "").strip())
    return canon(abs_url, keep_query=True)


def is_http_https(url: str) -> bool:
    try:
        scheme = urlparse(url).scheme.lower()
        return scheme in ("http", "https")
    except Exception:
        return False


def skip_youtube(url: str) -> bool:
    try:
        host = urlparse(url).netloc.lower()
        return ("youtube.com" in host) or ("youtu.be" in host)
    except Exception:
        return False


def contains_hou(url: str) -> bool:
    """Ch·ªâ ch·∫•p nh·∫≠n URL c√≥ ch·ªØ 'hou' (kh√¥ng ph√¢n bi·ªát hoa/th∆∞·ªùng)."""
    try:
        return "hou" in (url or "").lower()
    except Exception:
        return False


def is_crawlable(url: str) -> bool:
    """Lo·∫°i b·ªè scheme kh√¥ng h·ª£p l·ªá/JS/mail/fragment."""
    if not url:
        return False
    low = url.lower()
    if low.startswith(("mailto:", "javascript:", "#")):
        return False
    return True

# ================== X·ª≠ l√Ω n·ªôi dung ==================

def clean_html_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "iframe"]):
        tag.decompose()
    return soup.get_text(separator="\n", strip=True)


def number_lines(text: str) -> str:
    """ƒê√°nh s·ªë 1., 2., ... tr∆∞·ªõc m·ªói d√≤ng kh√¥ng r·ªóng; gi·ªØ d√≤ng tr·ªëng n·∫øu c√≥."""
    lines = (text or "").splitlines()
    out, idx = [], 1
    for line in lines:
        if line.strip():
            out.append(f"{idx}. {line}")
            idx += 1
        else:
            out.append("")
    return "\n".join(out)

# ================== Crawl b√†i vi·∫øt ==================

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36 HouCrawler/1.0"
}


def crawl_article(url: str, category_path, depth=0):
    indent = "  " * depth
    cu = canon(url, keep_query=True)

    if cu in visited:
        log(f"{indent}üîÅ B·ªè qua (ƒë√£ thƒÉm): {cu}")
        return

    # B·ªè qua n·∫øu URL ƒë√£ c√≥ trong d·ªØ li·ªáu tr∆∞·ªõc ƒë√≥
    if cu in existing_urls:
        visited.add(cu)
        log(f"{indent}‚è≠Ô∏è B·ªè qua (ƒë√£ c√≥ trong d·ªØ li·ªáu): {cu}")
        return

    visited.add(cu)
    try:
        log(f"{indent}üìù B·∫Øt ƒë·∫ßu crawl b√†i: {cu}")
        resp = requests.get(cu, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        title_tag = soup.find("h1") or soup.find("title")
        title = title_tag.get_text(strip=True) if title_tag else "Kh√¥ng r√µ ti√™u ƒë·ªÅ"

        date_tag = soup.find("time") or soup.find("div", class_="post-date")
        date = date_tag.get_text(strip=True) if date_tag else None

        content_div = (
            soup.find("div", class_="entry-content") or
            soup.find("div", class_="post-content") or
            soup.find("article") or
            soup.find("body")
        )
        if not content_div:
            log(f"{indent}‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung: {cu}")
            return

        content_html = str(content_div)
        content_cleaned = clean_html_text(content_html)
        if not content_cleaned.strip():
            log(f"{indent}‚ö†Ô∏è N·ªôi dung r·ªóng, b·ªè qua: {cu}")
            return

        content_numbered = number_lines(content_cleaned)

        article_data = {
            "title": title,
            "url": cu,                    # canonical (gi·ªØ query)
            "final_url": canon(resp.url, keep_query=True),
            "date": date,
            "category_path": category_path,
            "content_html": content_html,
            "content_cleaned": content_numbered,
        }

        articles_new.append(article_data)
        existing_urls.add(cu)  # tr√°nh tr√πng trong c√πng phi√™n
        log(f"{indent}‚úÖ ƒê√£ crawl xong: {title} | {cu}")

    except Exception as e:
        log(f"{indent}‚ùå L·ªói khi crawl b√†i: {cu} | {e}")

# ================== Crawl theo c√¢y ==================

def crawl_tree(url: str, category_name: str, parent_path, depth=0):
    indent = "  " * depth
    cu = canon(url, keep_query=True)

    if cu in visited:
        log(f"{indent}üîÅ B·ªè qua node (ƒë√£ thƒÉm): {cu}")
        return
    if not is_crawlable(cu):
        log(f"{indent}‚õî B·ªè qua (kh√¥ng crawlable): {cu}")
        return
    if not is_http_https(cu):
        log(f"{indent}‚õî B·ªè qua (kh√¥ng ph·∫£i http/https): {cu}")
        return
    if skip_youtube(cu):
        log(f"{indent}‚õî B·ªè qua (YouTube): {cu}")
        return
    if not contains_hou(cu):
        log(f"{indent}‚õî B·ªè qua (kh√¥ng ch·ª©a 'hou'): {cu}")
        return

    visited.add(cu)
    log(f"{indent}üìÇ Duy·ªát menu: {cu}")

    try:
        resp = requests.get(cu, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        for a in soup.find_all("a", href=True):
            raw = a['href'].strip()
            if not is_crawlable(raw):
                continue

            href = normalize_url(raw, resp.url)  # absolute + canonical

            if not is_http_https(href) or skip_youtube(href) or not contains_hou(href):
                continue
            if href in visited:
                continue

            title = a.get_text(strip=True) or category_name

            # Heuristic ph√¢n bi·ªát b√†i vi·∫øt / menu
            if "category" not in href and "page" not in href and len(href) > 20:
                log(f"{indent}  ‚ûú Ph√°t hi·ªán b√†i vi·∫øt: {href}")
                crawl_article(href, parent_path + [category_name], depth + 1)
            else:
                log(f"{indent}  ‚ûú ƒêi s√¢u menu: {href}")
                crawl_tree(href, title, parent_path + [category_name], depth + 1)

    except Exception as e:
        log(f"{indent}‚ùå L·ªói khi duy·ªát menu: {cu} | {e}")

# ================== main ==================

def main():
    INPUT = "../data/menu_links.json"
    OUTPUT_ARTICLES = "../data/menu_content.json"  # theo y√™u c·∫ßu

    # T·∫£i d·ªØ li·ªáu ƒë√£ c√≥ (n·∫øu t·ªìn t·∫°i) ƒë·ªÉ kh√¥ng crawl l·∫°i & KH√îNG ghi ƒë√®
    existing_articles = []
    if os.path.exists(OUTPUT_ARTICLES):
        try:
            with open(OUTPUT_ARTICLES, 'r', encoding='utf-8') as f:
                existing_articles = json.load(f)
            for it in existing_articles:
                if isinstance(it, dict):
                    u = it.get("url") or it.get("final_url")
                    if u:
                        existing_urls.add(canon(u, keep_query=True))
            log(f"üì¶ N·∫°p d·ªØ li·ªáu c≈©: {len(existing_articles)} b√†i.")
        except Exception as e:
            log(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c d·ªØ li·ªáu c≈©, s·∫Ω t·∫°o l·∫°i: {e}")
            existing_articles = []
            existing_urls.clear()

    # N·∫°p menu link g·ªëc
    with open(INPUT, 'r', encoding='utf-8') as f:
        menu_links = json.load(f)

    for item in menu_links:
        root_url = canon((item.get("url") or "").strip(), keep_query=True)
        root_name = (item.get("category") or "").strip() or "ROOT"
        if not is_crawlable(root_url):
            log(f"‚õî B·ªè qua root kh√¥ng crawlable: {root_url}")
            continue
        if not is_http_https(root_url):
            log(f"‚õî B·ªè qua root (kh√¥ng ph·∫£i http/https): {root_url}")
            continue
        if skip_youtube(root_url):
            log(f"‚õî B·ªè qua root (YouTube): {root_url}")
            continue
        if not contains_hou(root_url):
            log(f"‚õî B·ªè qua root (kh√¥ng ch·ª©a 'hou'): {root_url}")
            continue

        log(f"\nüåê B·∫Øt ƒë·∫ßu t·ª´: {root_name} | {root_url}")
        crawl_tree(root_url, root_name, [], depth=0)

    # H·ª¢P NH·∫§T: KH√îNG ghi ƒë√® d·ªØ li·ªáu ƒë√£ c√≥ ‚Äî ch·ªâ th√™m b√†i m·ªõi
    existing_set = {canon((it.get("url") or it.get("final_url") or ""), keep_query=True)
                    for it in existing_articles if isinstance(it, dict)}

    unique_new = []
    for it in articles_new:
        cu = canon(it.get("url") or it.get("final_url") or "", keep_query=True)
        if cu and cu not in existing_set:
            unique_new.append(it)
            existing_set.add(cu)

    merged_list = existing_articles + unique_new

    os.makedirs(os.path.dirname(OUTPUT_ARTICLES), exist_ok=True)
    with open(OUTPUT_ARTICLES, 'w', encoding='utf-8') as f:
        json.dump(merged_list, f, ensure_ascii=False, indent=2)

    log(f"\n‚úÖ ƒê√£ l∆∞u t·ªïng c·ªông {len(merged_list)} b√†i vi·∫øt t·∫°i: {OUTPUT_ARTICLES}")
    log(f"‚ûï M·ªõi th√™m trong l·∫ßn ch·∫°y n√†y: {len(unique_new)} (ƒë√£ b·ªè qua b√†i tr√πng)")


if __name__ == "__main__":
    main()
