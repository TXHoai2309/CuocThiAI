import json
import os
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# ƒê∆∞·ªùng d·∫´n file JSON ƒë·∫ßu v√†o
INPUT_FILE = "D:/airdrop/CuocThiAI/hou_crawler/crawler/data/menu_contents_refined.json"
OUTPUT_INDEX_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), 'hou_crawler', 'crawler', 'data', 'hou_index'))

# T·∫£i v√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh list Document
def load_documents(json_path):
    documents = []
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    for item in data:
        content = item.get("content", "").strip()
        if not content:
            continue

        title = (item.get("title") or "").lower()
        url = (item.get("url") or "").lower()
        category = (item.get("category") or "").lower()

        # G√°n section theo logic ƒë∆°n gi·∫£n
        if "tuy·ªÉn sinh" in title or "tuy·ªÉn sinh" in url:
            section = "tuy·ªÉn sinh"
        elif "ng√†nh" in title or "ng√†nh h·ªçc" in category:
            section = "ng√†nh h·ªçc"
        elif "s·ª± ki·ªán" in category:
            section = "s·ª± ki·ªán"
        elif "h·ªçc ph√≠" in category:
            section = "h·ªçc ph√≠"
        elif "h·ª£p t√°c" in category:
            section = "h·ª£p t√°c"
        elif "th√¥ng b√°o" in category:
            section = "th√¥ng b√°o"
        else:
            section = "kh√°c"

        doc = Document(
            page_content=content,
            metadata={
                "title": item.get("title", ""),
                "url": item.get("url", ""),
                "category": item.get("category", ""),
                "section": section
            }
        )
        documents.append(doc)

    return documents

def main():
    print("üì• ƒêang t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu JSON...")
    documents = load_documents(INPUT_FILE)

    print("üî™ Chia nh·ªè t√†i li·ªáu b·∫±ng RecursiveCharacterTextSplitter...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = splitter.split_documents(documents)

    print(f"üìÑ T·ªïng s·ªë ƒëo·∫°n sau chia: {len(split_docs)}")

    print("üß† ƒêang nh√∫ng d·ªØ li·ªáu v·ªõi HuggingFaceEmbeddings...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectordb = FAISS.from_documents(split_docs, embedding_model)

    print(f"üíæ L∆∞u FAISS index v√†o th∆∞ m·ª•c: {OUTPUT_INDEX_DIR}")
    vectordb.save_local(OUTPUT_INDEX_DIR)
    print("‚úÖ Ho√†n t·∫•t!")

if __name__ == "__main__":
    main()
