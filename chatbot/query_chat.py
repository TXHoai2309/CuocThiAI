import os
from typing import TypedDict, List
from click import prompt
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate

# CÃ i Ä‘áº·t API key cá»§a Google
os.environ["GOOGLE_API_KEY"] = "AIzaSyC-AUp7NplKO6Y1RRtjwdSu6tRe2aqsknU"  # Thay báº±ng API key cá»§a báº¡n

# ÄÆ°á»ng dáº«n Ä‘áº¿n file JSON chá»©a dá»¯ liá»‡u Ä‘Ã£ tinh chá»‰nh
JSON_DATA_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'hou_crawler', 'crawler', 'data', 'menu_contents_refined.json'))

# HÃ m táº£i dá»¯ liá»‡u tá»« file JSON
def load_json_data(json_path):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"Lá»—i khi táº£i file JSON: {e}")
        return None

# Táº£i FAISS vector store
try:
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        # ÄÆ°á»ng dáº«n tuyá»‡t Ä‘á»‘i Ä‘áº¿n thÆ° má»¥c chá»©a index.faiss vÃ  index.pkl
    # ÄÆ°á»ng dáº«n tuyá»‡t Ä‘á»‘i tá»›i THÆ¯ Má»¤C chá»©a index.faiss vÃ  index.pkl
    INDEX_DIR = r"D:\airdrop\CuocThiAI\hou_crawler\crawler\data\hou_index"  # âœ… ÄÃ¢y lÃ  thÆ° má»¥c

    print(f"ğŸ‘‰ Äang táº£i FAISS tá»«: {INDEX_DIR}")

    vectordb = FAISS.load_local(
        INDEX_DIR,  # âœ… Pháº£i lÃ  thÆ° má»¥c, khÃ´ng pháº£i file
        embeddings=embedding_model,
        index_name="index",  # TÆ°Æ¡ng á»©ng vá»›i: index.faiss + index.pkl
        allow_dangerous_deserialization=True
    )


except Exception as e:
    print(f"Lá»—i khi táº£i FAISS vector store: {e}") 
    exit(1)

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh Gemini
try:
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",  # Hoáº·c "gemini-1.5-pro" náº¿u báº¡n cÃ³ quyá»n truy cáº­p
        google_api_key=os.environ["GOOGLE_API_KEY"],
        temperature=0.5,
        max_output_tokens=512
    )
except Exception as e:
    print(f"Lá»—i khi khá»Ÿi táº¡o mÃ´ hÃ¬nh Gemini: {e}")
    exit(1)

# === Prompt tráº£ lá»i chÃ­nh ===
main_prompt = PromptTemplate.from_template("""Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh, tráº£ lá»i cÃ¢u há»i dá»±a trÃªn dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tinh chá»‰nh. HÃ£y tá»•ng há»£p vÃ  cung cáº¥p cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c, logic, dá»… hiá»ƒu vÃ  sÃ¡ng táº¡o náº¿u cáº§n dá»±a trÃªn thÃ´ng tin tá»« tÃ i liá»‡u sau:

**TÃ i liá»‡u**: {context}

**CÃ¢u há»i**: {question}

**Tráº£ lá»i**:""")

# === Prompt phÃ¢n loáº¡i section ===
section_prompt = PromptTemplate.from_template(
    """PhÃ¢n tÃ­ch cÃ¢u há»i sau vÃ  tráº£ lá»i báº±ng Ä‘Ãºng 1 tá»« trong: 
'giá»›i thiá»‡u', 'tuyá»ƒn sinh', 'ngÃ nh há»c', 'sá»± kiá»‡n', 'thÃ´ng bÃ¡o', 'há»£p tÃ¡c', 'há»c phÃ­', 'khÃ¡c'.\n\nCÃ¢u há»i: {query}\nPhÃ¢n loáº¡i:"""
)
section_chain = section_prompt | llm

# === Khai bÃ¡o tráº¡ng thÃ¡i LangGraph ===
class QAState(TypedDict):
    query: str
    section: str
    documents: List[Document]
    answer: str

# === Node 1: PhÃ¢n loáº¡i section ===
def classify_section(state: QAState) -> QAState:
    section = section_chain.invoke({"query": state["query"]}).content.strip().lower()
    if section not in ["giá»›i thiá»‡u", "tuyá»ƒn sinh", "ngÃ nh há»c", "sá»± kiá»‡n", "thÃ´ng bÃ¡o", "há»£p tÃ¡c", "há»c phÃ­", "khÃ¡c"]:
        section = "khÃ¡c"
    print(f"ğŸ“ Gemini xÃ¡c Ä‘á»‹nh section: {section}")
    return {**state, "section": section}


# === Node 2: Truy xuáº¥t tÃ i liá»‡u tá»« FAISS ===
def retrieve_docs(state: QAState) -> QAState:
    # Thá»­ tÃ¬m theo section náº¿u cÃ³ metadata
    try:
        docs = vectordb.max_marginal_relevance_search(
            query=state["query"],
            k=100,
            fetch_k=100,
            lambda_mult=0.5,
            filter=lambda meta: state["section"] in meta.get("section", "")
        )
    except:
        docs = []  # náº¿u FAISS khÃ´ng há»— trá»£ filter, fallback

    # Náº¿u khÃ´ng cÃ³ káº¿t quáº£, fallback: khÃ´ng dÃ¹ng filter
    if not docs:
        docs = vectordb.max_marginal_relevance_search(
            query=state["query"],
            k=100,
            fetch_k=100,
            lambda_mult=0.5
        )
        print("âš ï¸ KhÃ´ng tÃ¬m Ä‘Æ°á»£c tÃ i liá»‡u theo section, dÃ¹ng toÃ n bá»™ FAISS.")

    return {**state, "documents": docs}



# === Node 3: Táº¡o cÃ¢u tráº£ lá»i tá»« context ===
def truncate_docs(documents, max_chars=6000):
    combined = ""
    for doc in documents:
        if len(combined) + len(doc.page_content) < max_chars:
            combined += doc.page_content + "\n\n"
        else:
            break
    return combined

def generate_answer(state: QAState) -> QAState:
    docs_text = truncate_docs(state["documents"], max_chars=4000)  # Giáº£m max_chars
    print("ğŸ“‘ Context length:", len(docs_text))
    print("ğŸ“‘ Context sample:", docs_text[:500])
    if not docs_text.strip():
        print("âš ï¸ Empty context detected!")
        return {**state, "answer": "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan."}
    
    prompt = main_prompt.format(context=docs_text, question=state["query"])
    print("ğŸ“ Prompt sent to Gemini:", prompt[:500])
    
    try:
        response = llm.invoke(prompt)
        print("ğŸ“¤ Gemini raw response:", response)
        print("ğŸ“¤ Gemini response content:", response.content)
        if not response.content.strip():
            print("âš ï¸ Gemini returned empty response!")
            return {**state, "answer": "KhÃ´ng cÃ³ thÃ´ng tin phÃ¹ há»£p hoáº·c lá»—i tá»« Gemini."}
    except Exception as e:
        print("âŒ Gemini error:", str(e))
        return {**state, "answer": "KhÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i do lá»—i API."}
    
    return {**state, "answer": response.content}



# === XÃ¢y LangGraph ===
graph = StateGraph(QAState)
graph.add_node("classify", classify_section)
graph.add_node("retrieve", retrieve_docs)
graph.add_node("answer", generate_answer)

graph.set_entry_point("classify")
graph.add_edge("classify", "retrieve")
graph.add_edge("retrieve", "answer")
graph.set_finish_point("answer")

chatbot = graph.compile()

# === VÃ²ng láº·p CLI ===
while True:
    query = input("â“ Há»i: ").strip()
    if query.lower() in ["exit", "quit", "q"]:
        print("ğŸ‘‹ Táº¡m biá»‡t!")
        break

    result = chatbot.invoke({"query": query})
    print("\nğŸ“Œ Tráº£ lá»i:", result["answer"])
    print("\nğŸ“š Tá»« cÃ¡c Ä‘oáº¡n:")
    for doc in result["documents"]:
        print(f"- {doc.metadata.get('title', 'KhÃ´ng tiÃªu Ä‘á»')} | {doc.page_content[:100]}...")
