# chatbot/query_chat.py
import os
import re
from typing import TypedDict, List, Sequence
import secrets
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, START
from langchain.chains import LLMChain


# === API Key ===
os.environ["GOOGLE_API_KEY"] = "AIzaSyDR1eVkKtTN3RBeXNdW3bThRIwMMMfJND8"

# === Load FAISS ===
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

base_dir = os.path.dirname(os.path.abspath(__file__))
index_path = os.path.join(base_dir, "..", "hou_crawler", "crawler", "data", "hou_index")

vectordb = FAISS.load_local(
    index_path,
    embeddings=embedding_model,
    index_name="index",
    allow_dangerous_deserialization=True
)

# === Gemini model ===
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=os.environ["GOOGLE_API_KEY"],
    temperature=0.5,
    max_output_tokens=2048
)

# === Prompt tráº£ lá»i chÃ­nh ===
chat_main = ChatPromptTemplate.from_messages([
    ("system",
     "Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh, chá»‰ tráº£ lá»i dá»±a trÃªn dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tinh chá»‰nh. "
     "Æ¯u tiÃªn chÃ­nh xÃ¡c, logic, dá»… hiá»ƒu, vÃ  chá»‰ dÃ¹ng thÃ´ng tin trong pháº§n 'TÃ i liá»‡u'."
     "LuÃ´n hoÃ n thÃ nh cÃ¢u trá»n váº¹n, khÃ´ng bá» dá»Ÿ tá»«/cÃ¢u."),
    # ÄÆ°a context vÃ o dÆ°á»›i vai trÃ² system Ä‘á»ƒ mÃ´ hÃ¬nh xem Ä‘Ã¢y lÃ  nguá»“n sá»± tháº­t
    ("system", "TÃ i liá»‡u (context):\n{context}"),
    ("user", "{question}")
])
answer_chain = chat_main | llm  # pipe: prompt -> llm

# === Prompt phÃ¢n loáº¡i section ===
chat_section = ChatPromptTemplate.from_messages([
    ("system",
     "Báº¡n lÃ  bá»™ phÃ¢n loáº¡i Ã½ Ä‘á»‹nh. HÃ£y tráº£ vá» Ä‘Ãºng 1 tá»« trong: "
     "'giá»›i thiá»‡u', 'tuyá»ƒn sinh', 'ngÃ nh há»c', 'sá»± kiá»‡n', 'thÃ´ng bÃ¡o', 'há»£p tÃ¡c', 'há»c phÃ­', 'khÃ¡c'. "
     "KhÃ´ng thÃªm kÃ½ tá»± thá»«a."),
    ("user", "CÃ¢u há»i: {query}\nPhÃ¢n loáº¡i:")
])
section_chain = chat_section | llm  # pipe: prompt -> llm


# === Heuristics nháº­n diá»‡n cÃ¢u há»i fact + nÄƒm ===
FACT_KEYWORDS = [
    "Ä‘iá»ƒm sÃ n", "ngÆ°á»¡ng", "há»c phÃ­", "ngÃ y", "nÄƒm",
    "chá»‰ tiÃªu", "mÃ£", "Ä‘iá»ƒm", "thá»i háº¡n", "deadline", "tá»· lá»‡", "tá»‰ lá»‡", "bao nhiÃªu"
]
YEAR_RE = re.compile(r"(20\d{2})")

# === Khai bÃ¡o tráº¡ng thÃ¡i LangGraph ===
class QAState(TypedDict):
    query: str
    section: str
    documents: List[Document]
    answer: str

def is_fact_query(query: str) -> bool:
    """
    Má»¥c Ä‘Ã­ch: Nháº­n diá»‡n cÃ¢u há»i dáº¡ng fact/sá»‘ liá»‡u (cÃ³ nÄƒm, ngÃ y, con sá»‘...)
    -> dÃ¹ng retriever kiá»ƒu THRESHOLD Ä‘á»ƒ siáº¿t nhiá»…u cho chÃ­nh xÃ¡c hÆ¡n.
    """
    ql = query.lower()
    return any(keyword in ql for keyword in FACT_KEYWORDS) or bool(YEAR_RE.search(ql))

def year_priority_filter(query: str, docs: List[Document]) -> List[Document]:
    """
    Má»¥c Ä‘Ã­ch: Náº¿u query cÃ³ nÄƒm (20xx), Æ°u tiÃªn cÃ¡c doc cÃ³ nÄƒm Ä‘Ã³ trong
    content hoáº·c metadata (title/url/date/section/category).
    """
    m = YEAR_RE.search(query)
    if not m:
        return docs
    year = m.group(1)

    def has_year(d: Document) -> bool:
        meta = d.metadata or {}
        hay = (d.page_content or "") + " " + " ".join(
            str(meta.get(k, "")) for k in ["date", "title", "url", "section", "category"]
        )
        return year in hay

    with_year = [d for d in docs if has_year(d)]
    others = [d for d in docs if not has_year(d)]
    return with_year + others  # Æ°u tiÃªn khá»›p-nÄƒm náº±m trÆ°á»›c

def retrieve_with_threshold(query: str, min_score: float = 0.35, k_cap: int = 100) -> List[Document]:
    """
    Retriever kiá»ƒu 'lá»c theo ngÆ°á»¡ng' Ä‘á»ƒ tÄƒng precision cho cÃ¢u há»i fact.

    - Gá»i similarity_search_with_relevance_scores -> tráº£ (Document, score).
    - Lá»c nhá»¯ng doc cÃ³ score >= min_score.
    - Fallback: náº¿u sau lá»c cÃ²n quÃ¡ Ã­t (<4) -> tráº£ top 10 ban Ä‘áº§u (giá»¯ recall).
    """
    docs_scores = vectordb.similarity_search_with_relevance_scores(query, k=k_cap)
    filtered = [d for d, s in docs_scores if (s is None) or (s >= min_score)]
    if len(filtered) < 4:
        return [d for d, _ in docs_scores][:10]
    return filtered

# Má»¤C ÄÃCH: Truy xuáº¥t kiá»ƒu MMR = tÄƒng Ä‘a dáº¡ng, giáº£m trÃ¹ng láº·p (há»¯u Ã­ch cho cÃ¢u há»i khÃ¡i quÃ¡t)
def retrieve_with_mmr(query: str, section_value: str) -> List[Document]:
    """
    Æ¯u tiÃªn filter theo 'section' (náº¿u metadata cÃ³) Ä‘á»ƒ giáº£m nhiá»…u.
    Náº¿u backend khÃ´ng há»— trá»£ filter lambda -> tráº£ [] Ä‘á»ƒ node retrieve_docs fallback.
    """
    try:
        return vectordb.max_marginal_relevance_search(
            query=query,
            k=100,
            fetch_k=100,
            lambda_mult=0.5,
            filter=lambda meta: section_value in meta.get("section", "")
        )
    except Exception:
        return []  # Ä‘á»ƒ node retrieve_docs biáº¿t fallback

# === Node 1: PhÃ¢n loáº¡i section ===
def classify_section(state: QAState) -> QAState:
    section = section_chain.invoke({"query": state["query"]}).content.strip().lower()
    if section not in ["giá»›i thiá»‡u", "tuyá»ƒn sinh", "ngÃ nh há»c", "sá»± kiá»‡n", "thÃ´ng bÃ¡o", "há»£p tÃ¡c", "há»c phÃ­", "khÃ¡c"]:
        section = "khÃ¡c"
    print(f"ğŸ“ Gemini xÃ¡c Ä‘á»‹nh section: {section}")
    return {**state, "section": section}

# === Node 2: Truy xuáº¥t tÃ i liá»‡u tá»« FAISS ===
def retrieve_docs(state: QAState) -> QAState:
    """
    Má»¤C ÄÃCH NODE (Ä‘Ã£ chá»‰nh nháº¹):
    1) CÃ¢u há»i FACT -> dÃ¹ng THRESHOLD Ä‘á»ƒ SIáº¾T NHIá»„U (tÄƒng chÃ­nh xÃ¡c).
       - Náº¿u káº¿t quáº£ quÃ¡ Ã­t -> Fallback sang MMR Ä‘á»ƒ giá»¯ Ä‘á»™ bao phá»§.
    2) CÃ¢u há»i KHÃI QUÃT -> dÃ¹ng MMR Ä‘á»ƒ ÄA Dáº NG ngá»¯ cáº£nh (giáº£m trÃ¹ng láº·p).
    3) Sau khi cÃ³ danh sÃ¡ch docs -> Æ¯U TIÃŠN THEO NÄ‚M (náº¿u query cÃ³ nÄƒm).
       (ÄÃƒ Bá» bÆ°á»›c boost theo metadata nhÆ° báº¡n yÃªu cáº§u.)
    """
    query = state["query"]
    section_value = state["section"]

    # 1) Chá»n chiáº¿n lÆ°á»£c theo loáº¡i cÃ¢u há»i
    if is_fact_query(query):
        docs = retrieve_with_threshold(query, min_score=0.35, k_cap=100)
        if len(docs) < 4:  # fallback Ä‘á»ƒ khÃ´ng bá»‹ thiáº¿u ngá»¯ cáº£nh
            try:
                docs = vectordb.max_marginal_relevance_search(
                    query=query, k=100, fetch_k=100, lambda_mult=0.5,
                    filter=lambda meta: section_value in meta.get("section", "")
                )
            except Exception:
                docs = vectordb.max_marginal_relevance_search(
                    query=query, k=100, fetch_k=100, lambda_mult=0.5
                )
                print("âš ï¸ Threshold quÃ¡ gáº¯t hoáº·c khÃ´ng filter Ä‘Æ°á»£c theo section, fallback MMR toÃ n bá»™ FAISS.")
    else:
        # CÃ¢u há»i khÃ¡i quÃ¡t -> MMR (giáº£m trÃ¹ng láº·p)
        docs = retrieve_with_mmr(query, section_value)
        if not docs:
            # Fallback náº¿u filter lambda khÃ´ng cháº¡y Ä‘Æ°á»£c
            docs = vectordb.max_marginal_relevance_search(
                query=query, k=100, fetch_k=100, lambda_mult=0.5
            )
            print("âš ï¸ KhÃ´ng filter theo section Ä‘Æ°á»£c, dÃ¹ng MMR trÃªn toÃ n bá»™ FAISS.")

    # 2) Æ¯u tiÃªn cÃ¡c Ä‘oáº¡n cÃ¹ng NÄ‚M vá»›i query (náº¿u cÃ³)
    docs = year_priority_filter(query, docs)

    return {**state, "documents": docs}

# === Node 3: Táº¡o cÃ¢u tráº£ lá»i tá»« context ===
def truncate_docs(documents, max_chars=6000):
    """
    Má»¥c Ä‘Ã­ch: gom cÃ¡c Ä‘oáº¡n context Ä‘á»§ ngáº¯n (<= max_chars) Ä‘á»ƒ nhÃ©t vÃ o prompt,
    trÃ¡nh trÃ n token. Giá»¯ nguyÃªn thá»© tá»± Ä‘Ã£ chá»n.
    """
    combined = ""
    for doc in documents:
        if len(combined) + len(doc.page_content) < max_chars:
            combined += doc.page_content + "\n\n"
        else:
            break
    return combined

def generate_answer(state: QAState) -> QAState:
    docs_text = truncate_docs(state["documents"], max_chars=4000)  # ngÆ°á»¡ng 4k kÃ½ tá»± Ä‘á»ƒ giáº£m rá»§i ro trÃ n token
    if not docs_text.strip():
        print("âš ï¸ Empty context detected!")
        return {**state, "answer": "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan."}

    # Gá»i chain chuáº©n: truyá»n {context, question} vÃ o prompt -> LLM
    try:
        resp = answer_chain.invoke({"context": docs_text, "question": state["query"]})
        content = (resp.content or "").strip()
        if not content:
            return {**state, "answer": "KhÃ´ng cÃ³ thÃ´ng tin phÃ¹ há»£p hoáº·c lá»—i tá»« Gemini."}
        return {**state, "answer": content}
    except Exception as e:
        print("âŒ Gemini error:", str(e))
        return {**state, "answer": "KhÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i do lá»—i API."}

# === XÃ¢y LangGraph ===
graph = StateGraph(QAState)
graph.add_node("classify", classify_section)
graph.add_node("retrieve", retrieve_docs)
graph.add_node("answer", generate_answer)

graph.set_entry_point("classify")
graph.add_edge("classify", "retrieve")
graph.add_edge("retrieve", "answer")
graph.set_finish_point("answer")

# ... cÃ¡c pháº§n trÃªn giá»¯ nguyÃªn ...

chatbot = graph.compile()

# === VÃ²ng láº·p CLI chá»‰ cháº¡y khi cháº¡y trá»±c tiáº¿p file nÃ y ===
if __name__ == "__main__":
    while True:
        query = input("â“ Há»i: ").strip()
        if query.lower() in ["exit", "quit", "q"]:
            print("ğŸ‘‹ Táº¡m biá»‡t!")
            break
        result = chatbot.invoke({"query": query})
        print("\nğŸ“Œ Tráº£ lá»i:", result["answer"])
