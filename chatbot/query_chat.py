# chatbot/query_chat.py
import os
import re
from typing import Annotated, TypedDict, List, Sequence

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

# === API Key ===
os.environ.setdefault("GOOGLE_API_KEY", "AIzaSyBiVUeOqe12cgPIaphtiudVMEWyS9_mieo")

# === Load FAISS ===
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
base_dir = os.path.dirname(os.path.abspath(__file__))
index_path = os.path.join(base_dir, "..", "data", "hou", "hou_index")

vectordb = FAISS.load_local(
    index_path,
    embeddings=embedding_model,
    index_name="index",
    allow_dangerous_deserialization=True,
)

# === Gemini model ===
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=os.environ["GOOGLE_API_KEY"],
    temperature=0.5,
    max_output_tokens=2048,
)

# === Prompt tráº£ lá»i chÃ­nh (RAG) ===
chat_main = ChatPromptTemplate.from_messages([
    (
        "system",
        "Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh, chá»‰ tráº£ lá»i dá»±a trÃªn dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tinh chá»‰nh. "
        "Æ¯u tiÃªn chÃ­nh xÃ¡c, logic, dá»… hiá»ƒu, vÃ  chá»‰ dÃ¹ng thÃ´ng tin trong pháº§n 'TÃ i liá»‡u'."
        "vá»›i ngá»¯ cáº£nh há»™i thoáº¡i (vÃ­ dá»¥: tÃªn ngÆ°á»i dÃ¹ng, tham chiáº¿u nhÆ° 'ngÃ nh nÃ y'), báº¡n Ä‘Æ°á»£c phÃ©p dÃ¹ng lá»‹ch sá»­ há»™i thoáº¡i"
        "HÃ£y TRÃŒNH BÃ€Y Káº¾T QUáº¢ THEO MARKDOWN, ngáº¯n gá»n vÃ  rÃµ rÃ ng, Æ°u tiÃªn:\n"
        "1) TiÃªu Ä‘á» cáº¥p 3 trá»Ÿ xuá»‘ng (###),\n"
        "2) Danh sÃ¡ch gáº¡ch Ä‘áº§u dÃ²ng cho cÃ¡c Ã½ chÃ­nh,\n"
        "3) Báº£ng khi so sÃ¡nh/ liá»‡t kÃª theo cá»™t,\n"
        "4) Chia Ä‘oáº¡n ngáº¯n; khÃ´ng viáº¿t thÃ nh má»™t khá»‘i dÃ i.\n"
        "Æ¯u tiÃªn chÃ­nh xÃ¡c, logic, dá»… hiá»ƒu luÃ´n hoÃ n thÃ nh cÃ¢u trá»n váº¹n.",
    ),
    MessagesPlaceholder(variable_name="messages"),
    ("user",
     "Dá»±a trÃªn TÃ€I LIá»†U dÆ°á»›i Ä‘Ã¢y, hÃ£y tráº£ lá»i cÃ¢u há»i.\n\n"
     "TÃ€I LIá»†U:\n{context}\n\n"
     "CÃ‚U Há»I: {question}\n\nTráº£ lá»i:")
])
answer_chain = chat_main | llm

# === Prompt phÃ¢n loáº¡i section ===
chat_section = ChatPromptTemplate.from_messages([
    (
        "system",
        "Báº¡n lÃ  bá»™ phÃ¢n loáº¡i Ã½ Ä‘á»‹nh. HÃ£y tráº£ vá» Ä‘Ãºng 1 tá»« trong: "
        "'giá»›i thiá»‡u', 'tuyá»ƒn sinh', 'ngÃ nh há»c', 'sá»± kiá»‡n', 'thÃ´ng bÃ¡o', 'há»£p tÃ¡c', 'há»c phÃ­', 'khÃ¡c'. "
        "KhÃ´ng thÃªm kÃ½ tá»± thá»«a.",
    ),
    ("user", "CÃ¢u há»i: {query}\nPhÃ¢n loáº¡i:"),
])
section_chain = chat_section | llm

# === Heuristics nháº­n diá»‡n fact + nÄƒm ===
FACT_KEYWORDS = [
    "Ä‘iá»ƒm sÃ n",
    "ngÆ°á»¡ng",
    "há»c phÃ­",
    "ngÃ y",
    "nÄƒm",
    "chá»‰ tiÃªu",
    "mÃ£",
    "Ä‘iá»ƒm",
    "thá»i háº¡n",
    "deadline",
    "tá»· lá»‡",
    "tá»‰ lá»‡",
    "bao nhiÃªu",
]
YEAR_RE = re.compile(r"(20\d{2})")

# === Khai bÃ¡o STATE cÃ³ bá»™ nhá»› tin nháº¯n ===
class State(TypedDict):
    query: str
    section: str
    documents: List[Document]
    answer: str
    messages: Annotated[Sequence[BaseMessage], add_messages]  # [Sá»¬A] cho phÃ©p auto-append



def last_user_text(messages: Sequence[BaseMessage]) -> str:
    for m in reversed(messages):
        if isinstance(m, HumanMessage):
            return (m.content or "").strip()
    return ""


def is_fact_query(query: str) -> bool:
    ql = query.lower()
    return any(k in ql for k in FACT_KEYWORDS) or bool(YEAR_RE.search(ql))


def year_priority_filter(query: str, docs: List[Document]) -> List[Document]:
    m = YEAR_RE.search(query)
    if not m:
        return docs
    year = m.group(1)

    def has_year(d: Document) -> bool:
        meta = d.metadata or {}
        hay = (d.page_content or "") + " " + " ".join(
            str(meta.get(k, "")) for k in ["date", "title", "url", "section", "category"]
        )
        return year in hay

    with_year = [d for d in docs if has_year(d)]
    others = [d for d in docs if not has_year(d)]
    return with_year + others


def retrieve_with_threshold(query: str, min_score: float = 0.35, k_cap: int = 100) -> List[Document]:
    docs_scores = vectordb.similarity_search_with_relevance_scores(query, k=k_cap)
    filtered = [d for d, s in docs_scores if (s is None) or (s >= min_score)]
    if len(filtered) < 4:
        return [d for d, _ in docs_scores][:10]
    return filtered


def retrieve_with_mmr(query: str, section_value: str) -> List[Document]:
    try:
        return vectordb.max_marginal_relevance_search(
            query=query,
            k=100,
            fetch_k=100,
            lambda_mult=0.5,
            filter=lambda meta: section_value in meta.get("section", ""),
        )
    except Exception:
        return []

# === Node 1: phÃ¢n loáº¡i + rÃºt ra query tá»« messages ===

def classify_section(state: State) -> State:
    query = (state.get("query") or last_user_text(state.get("messages", [])) or "").strip()
    section = section_chain.invoke({"query": query}).content.strip().lower()
    if section not in [
        "giá»›i thiá»‡u",
        "tuyá»ƒn sinh",
        "ngÃ nh há»c",
        "sá»± kiá»‡n",
        "thÃ´ng bÃ¡o",
        "há»£p tÃ¡c",
        "há»c phÃ­",
        "khÃ¡c",
    ]:
        section = "khÃ¡c"
    print(f"ğŸ“ Gemini xÃ¡c Ä‘á»‹nh section: {section}")
    return {**state, "section": section}

# === Node 2: retrieve ===

def retrieve_docs(state: State) -> State:
    query = state["query"]
    section_value = state["section"]

    if is_fact_query(query):
        docs = retrieve_with_threshold(query, min_score=0.35, k_cap=100)
        if len(docs) < 4:
            try:
                docs = vectordb.max_marginal_relevance_search(
                    query=query,
                    k=100,
                    fetch_k=100,
                    lambda_mult=0.5,
                    filter=lambda meta: section_value in meta.get("section", ""),
                )
            except Exception:
                docs = vectordb.max_marginal_relevance_search(
                    query=query, k=100, fetch_k=100, lambda_mult=0.5
                )
                print("âš ï¸ Threshold gáº¯t hoáº·c khÃ´ng filter section, fallback MMR toÃ n bá»™.")
    else:
        docs = retrieve_with_mmr(query, section_value)
        if not docs:
            docs = vectordb.max_marginal_relevance_search(
                query=query, k=100, fetch_k=100, lambda_mult=0.5
            )
            print("âš ï¸ KhÃ´ng filter theo section Ä‘Æ°á»£c, dÃ¹ng MMR trÃªn toÃ n bá»™ FAISS.")

    docs = year_priority_filter(query, docs)
    return {**state, "documents": docs}

# === Node 3: tráº£ lá»i + Ä‘áº©y AIMessage vÃ o bá»™ nhá»› ===

def truncate_docs(documents: List[Document], max_chars=4000) -> str:
    combined = ""
    for doc in documents:
        if len(combined) + len(doc.page_content) < max_chars:
            combined += doc.page_content + "\n\n"
        else:
            break
    return combined


def generate_answer(state: State) -> State:
    docs_text = truncate_docs(state["documents"], max_chars=4000)
    if not docs_text.strip():
        content = "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan."
        return {**state, "messages": [AIMessage(content=content)], "answer": content}

    try:
        # [Sá»¬A] Truyá»n cáº£ history
        resp = answer_chain.invoke({"context": docs_text, "messages": state["messages"],"question": state["query"],})
        content = (resp.content or "").strip() or "KhÃ´ng cÃ³ thÃ´ng tin phÃ¹ há»£p hoáº·c lá»—i tá»« Gemini."
        return {**state, "messages": [AIMessage(content=content)], "answer": content}
    except Exception:
        content = "KhÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i do lá»—i API."
        return {**state, "messages": [AIMessage(content=content)], "answer": content}



def add_user_message(state: State) -> State:
    return {**state, "messages": [HumanMessage(content=state["query"])]}

# === XÃ¢y graph cÃ³ checkpointer (MemorySaver) ===
graph = StateGraph(State)

graph.add_node("add_user_message", add_user_message)

graph.add_node("classify", classify_section)
graph.add_node("retrieve", retrieve_docs)
graph.add_node("answer", generate_answer)

graph.set_entry_point("add_user_message")
graph.add_edge("add_user_message", "classify")
graph.add_edge("classify", "retrieve")
graph.add_edge("retrieve", "answer")
graph.set_finish_point("answer")

# Báº¬T Bá»˜ NHá»š: má»—i thread_id sáº½ cÃ³ lá»‹ch sá»­ messages riÃªng
memory = MemorySaver()
chatbot = graph.compile(checkpointer=memory)

# === CLI test khi cháº¡y trá»±c tiáº¿p ===
if __name__ == "__main__":
    cfg = {"configurable": {"thread_id": "demo"}}
    while True:
        q = input("â“ Há»i: ").strip()
        if q.lower() in ["exit", "quit", "q"]:
            break
        # [Sá»¬A] gá»­i query Ä‘á»ƒ node add_user_message thÃªm HumanMessage
        result = chatbot.invoke({"query": q}, cfg)
        print("\nğŸ“Œ Tráº£ lá»i:", result.get("answer"))
