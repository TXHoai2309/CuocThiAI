import json
import os
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate

# C√†i ƒë·∫∑t API key c·ªßa Google
os.environ["GOOGLE_API_KEY"] = "AIzaSyC-AUp7NplKO6Y1RRtjwdSu6tRe2aqsknU"  # Thay b·∫±ng API key c·ªßa b·∫°n

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON ch·ª©a d·ªØ li·ªáu ƒë√£ tinh ch·ªânh
JSON_DATA_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'hou_crawler', 'crawler', 'data', 'menu_contents_refined.json'))

# H√†m t·∫£i d·ªØ li·ªáu t·ª´ file JSON
def load_json_data(json_path):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"L·ªói khi t·∫£i file JSON: {e}")
        return None

# T·∫£i FAISS vector store
try:
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        # ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ƒë·∫øn th∆∞ m·ª•c ch·ª©a index.faiss v√† index.pkl
    # ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi t·ªõi TH∆Ø M·ª§C ch·ª©a index.faiss v√† index.pkl
    INDEX_DIR = r"D:\airdrop\CuocThiAI\hou_crawler\crawler\data\hou_index"  # ‚úÖ ƒê√¢y l√† th∆∞ m·ª•c

    print(f"üëâ ƒêang t·∫£i FAISS t·ª´: {INDEX_DIR}")

    vectordb = FAISS.load_local(
        INDEX_DIR,  # ‚úÖ Ph·∫£i l√† th∆∞ m·ª•c, kh√¥ng ph·∫£i file
        embeddings=embedding_model,
        index_name="index",  # T∆∞∆°ng ·ª©ng v·ªõi: index.faiss + index.pkl
        allow_dangerous_deserialization=True
    )


except Exception as e:
    print(f"L·ªói khi t·∫£i FAISS vector store: {e}") 
    exit(1)

# Kh·ªüi t·∫°o m√¥ h√¨nh Gemini
try:
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",  # Ho·∫∑c "gemini-1.5-pro" n·∫øu b·∫°n c√≥ quy·ªÅn truy c·∫≠p
        google_api_key=os.environ["GOOGLE_API_KEY"],
        temperature=0.5,
        max_output_tokens=512
    )
except Exception as e:
    print(f"L·ªói khi kh·ªüi t·∫°o m√¥ h√¨nh Gemini: {e}")
    exit(1)

# T·∫°o prompt template ƒë·ªÉ c·∫£i thi·ªán ng·ªØ c·∫£nh tr·∫£ l·ªùi
prompt_template = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c tinh ch·ªânh. H√£y cung c·∫•p c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn v√† d·ªÖ hi·ªÉu d·ª±a tr√™n th√¥ng tin t·ª´ t√†i li·ªáu sau:

**T√†i li·ªáu**: {context}

**C√¢u h·ªèi**: {question}

**Tr·∫£ l·ªùi**: """
prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# T·∫°o QA chain v·ªõi prompt t√πy ch·ªânh
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectordb.as_retriever(search_kwargs={"k": 5}),
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt, "document_variable_name": "context"}
)

# T·∫£i d·ªØ li·ªáu JSON ƒë·ªÉ cung c·∫•p ng·ªØ c·∫£nh b·ªï sung (n·∫øu c·∫ßn)
json_data = load_json_data(JSON_DATA_PATH)
if json_data is None:
    print("Kh√¥ng th·ªÉ ti·∫øp t·ª•c v√¨ kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu JSON.")
    exit(1)

# V√≤ng l·∫∑p h·ªèi ƒë√°p
while True:
    try:
        query = input("‚ùì H·ªèi: ").strip()
        if query.lower() in ["exit", "quit", "q"]:
            print("Tho√°t ch∆∞∆°ng tr√¨nh.")
            break
        response = qa_chain.invoke({"query": query})
        print("\nüìå Tr·∫£ l·ªùi:", response["result"])
        # Hi·ªÉn th·ªã ngu·ªìn t√†i li·ªáu (t√πy ch·ªçn)
        print("\nüìö Ngu·ªìn t√†i li·ªáu:")
        for doc in response["source_documents"]:
            print(f"- {doc.page_content[:100]}...")
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {e}")