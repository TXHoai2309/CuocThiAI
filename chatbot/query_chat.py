# chatbot/query_chat.py
import os
import re
import unicodedata  # [NEW] d√πng ƒë·ªÉ chu·∫©n h√≥a b·ªè d·∫•u ti·∫øng Vi·ªát
from typing import Annotated, TypedDict, List, Sequence

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

# === API Key ===
os.environ.setdefault("GOOGLE_API_KEY", "AIzaSyDR1eVkKtTN3RBeXNdW3bThRIwMMMfJND8")

# === Load FAISS ===

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
base_dir = os.path.dirname(os.path.abspath(__file__))

embedding_model_ctdt = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn index FAISS
hou_index_path = os.path.join(base_dir, "..", "data", "hou", "hou_index")
diem_index_path = os.path.join(base_dir, "..", "data", "hou", "diem_chuan_index")

ctdt_index_path = os.path.join(base_dir, "..", "data", "hou", "ctdt_index")
if not os.path.exists(ctdt_index_path):  # [NEW] fallback path
    ctdt_index_path = os.path.join(base_dir, "..", "data", "ctdt_index")

# T·∫£i index FAISS v·ªõi embeddings
vectordb_hou = FAISS.load_local(
    hou_index_path,
    embeddings=embedding_model,
    index_name="index",
    allow_dangerous_deserialization=True,
)

vectordb_diem = FAISS.load_local(
    diem_index_path,
    embeddings=embedding_model,
    index_name="index",
    allow_dangerous_deserialization=True,
)

vectordb_ctdt = FAISS.load_local(
    ctdt_index_path,
    embeddings=embedding_model_ctdt,
    index_name="index",  # m·∫∑c ƒë·ªãnh c·ªßa save_local
    allow_dangerous_deserialization=True,
)

# [CHANGE] Danh s√°ch kho s·∫Ω ƒë∆∞·ª£c router c·∫≠p nh·∫≠t theo truy v·∫•n
selected_vector_stores: List[FAISS] = [vectordb_hou]

# === Gemini model ===
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=os.environ["GOOGLE_API_KEY"],
    temperature=0.5,
    max_output_tokens=2048,
)

# === Prompt tr·∫£ l·ªùi ch√≠nh (RAG) ===
chat_main = ChatPromptTemplate.from_messages([
    (
        "system",
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c tinh ch·ªânh. "
        "∆Øu ti√™n ch√≠nh x√°c, logic, d·ªÖ hi·ªÉu, v√† ch·ªâ d√πng th√¥ng tin trong ph·∫ßn 'T√†i li·ªáu'. "
        "V·ªõi ng·ªØ c·∫£nh h·ªôi tho·∫°i (v√≠ d·ª•: t√™n ng∆∞·ªùi d√πng, tham chi·∫øu nh∆∞ 'ng√†nh n√†y'), b·∫°n ƒë∆∞·ª£c ph√©p d√πng l·ªãch s·ª≠ h·ªôi tho·∫°i. "
        "H√£y TR√åNH B√ÄY K·∫æT QU·∫¢ THEO MARKDOWN, ng·∫Øn g·ªçn v√† r√µ r√†ng, ∆∞u ti√™n:\n"
        "1) Ti√™u ƒë·ªÅ c·∫•p 3 tr·ªü xu·ªëng (###),\n"
        "2) Danh s√°ch g·∫°ch ƒë·∫ßu d√≤ng cho c√°c √Ω ch√≠nh,\n"
        "3) B·∫£ng khi so s√°nh/ li·ªát k√™ theo c·ªôt,\n"
        "4) Chia ƒëo·∫°n ng·∫Øn; kh√¥ng vi·∫øt th√†nh m·ªôt kh·ªëi d√†i.\n"
        "∆Øu ti√™n ch√≠nh x√°c, logic, d·ªÖ hi·ªÉu lu√¥n ho√†n th√†nh c√¢u tr·ªçn v·∫πn.",
    ),
    MessagesPlaceholder(variable_name="messages"),
    ("user",
     "D·ª±a tr√™n T√ÄI LI·ªÜU d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi.\n\n"
     "T√ÄI LI·ªÜU:\n{context}\n\n"
     "C√ÇU H·ªéI: {question}\n\nTr·∫£ l·ªùi:")
])
answer_chain = chat_main | llm

# === Prompt ph√¢n lo·∫°i section ===
chat_section = ChatPromptTemplate.from_messages([
    (
        "system",
        "B·∫°n l√† b·ªô ph√¢n lo·∫°i √Ω ƒë·ªãnh. H√£y tr·∫£ v·ªÅ ƒë√∫ng 1 t·ª´ trong: "
        "'gi·ªõi thi·ªáu', 'tuy·ªÉn sinh', 'ng√†nh h·ªçc', 's·ª± ki·ªán', 'th√¥ng b√°o', 'h·ª£p t√°c', 'h·ªçc ph√≠', 'kh√°c'. "
        "Kh√¥ng th√™m k√Ω t·ª± th·ª´a.",
    ),
    ("user", "C√¢u h·ªèi: {query}\nPh√¢n lo·∫°i:"),
])
section_chain = chat_section | llm

YEAR_RE = re.compile(r"(20\d{2})")


# [NEW] Chu·∫©n h√≥a ti·∫øng Vi·ªát: lowercase + b·ªè d·∫•u + thay '_' th√†nh ' ' + g·ªçn kho·∫£ng tr·∫Øng
def vn_normalize(s: str) -> str:
    s = (s or "").lower().strip().replace("_", " ")
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    s = unicodedata.normalize("NFC", s)
    s = re.sub(r"\s+", " ", s)
    return s

# [NEW] B·ªô keyword m·ªü r·ªông (kh√¥ng d·∫•u) cho 3 kho v√† b·ªô ph·ªß ƒë·ªãnh ƒë·ªÉ gi·∫£m nh·∫ßm l·∫´n
CTDT_KEYWORDS_RAW = [
    # Thu·∫≠t ng·ªØ chung v·ªÅ ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o
    "ctƒët", "ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o", "khung ch∆∞∆°ng tr√¨nh", "k·∫ø ho·∫°ch ƒë√†o t·∫°o", "k·∫ø ho·∫°ch h·ªçc t·∫≠p",
    "ch∆∞∆°ng tr√¨nh h·ªçc", "kh·ªëi ki·∫øn th·ª©c", "kh·ªëi_ki·∫øn_th·ª©c", "kh·ªëi ng√†nh", "chuy√™n ng√†nh", "ng√†nh h·ªçc",
    # Th√†nh ph·∫ßn v√† h·ªçc ph·∫ßn
    "h·ªçc ph·∫ßn", "m√¥n h·ªçc", "m√¥n", "m√¥ t·∫£ h·ªçc ph·∫ßn", "h·ªçc ph·∫ßn b·∫Øt bu·ªôc", "h·ªçc ph·∫ßn t·ª± ch·ªçn",
    "h·ªçc ph·∫ßn t·ª± ch·ªçn t·ª± do", "s·ªë t√≠n ch·ªâ", "t√≠n ch·ªâ", "tin chi", "th·ªùi l∆∞·ª£ng", "h·ªçc k·ª≥",
    # Chu·∫©n ƒë·∫ßu ra
    "clo", "plo", "chu·∫©n ƒë·∫ßu ra", "chu·∫©n ƒë·∫ßu ra h·ªçc ph·∫ßn", "chu·∫©n ƒë·∫ßu ra ch∆∞∆°ng tr√¨nh", "m·ª•c ti√™u ƒë√†o t·∫°o",
    # Th√¥ng tin b·ªï sung
    "th·ªùi gian ƒë√†o t·∫°o", "th·ªùi_gian_ƒë√†o_t·∫°o", "v·ªã tr√≠ vi·ªác l√†m", "v·ªã_tr√≠_vi·ªác_l√†m",
    "c∆° h·ªôi ngh·ªÅ nghi·ªáp", "y√™u c·∫ßu ƒë·∫ßu v√†o", "ƒëi·ªÅu ki·ªán ti√™n quy·∫øt",
]
DIEM_KEYWORDS_RAW = [
    "ƒëi·ªÉm", "ƒëi·ªÉm chu·∫©n", "ƒëi·ªÉm s√†n", "ng∆∞·ª°ng", "ƒëi·ªÉm x√©t tuy·ªÉn", "ch·ªâ ti√™u", "m√£ ng√†nh", "t·ªï h·ª£p",
    "ƒëi·ªÉm thi", "ƒëi·ªÉm h·ªçc b·∫°", "ƒëi·ªÉm ƒë√°nh gi√° nƒÉng l·ª±c", "ƒëi·ªÉm t∆∞ duy", "x√©t tuy·ªÉn", "thpt", "h·ªçc b·∫°", "ƒëgnl", "ƒëgtd",
    "a00", "a01", "b00", "d01", "d07",
]
# [FIX] Thi·∫øu d·∫•u ph·∫©y gi·ªØa "tuy·ªÉn sinh" v√† "chuy√™n ng√†nh" -> g√¢y d√≠nh chu·ªói
HOU_KEYWORDS_RAW = [
    "h·ªçc ph√≠", "th√¥ng b√°o", "tin t·ª©c", "s·ª± ki·ªán", "tuy·ªÉn sinh",
    "gi·ªõi thi·ªáu", "h·ª£p t√°c", "ƒë·ªëi t√°c",
    "khoa", "vi·ªán", "ph√≤ng ban", "b·ªô m√¥n", "gi·∫£ng vi√™n", "c√°n b·ªô", "h·ªì s∆°", "th·ªß t·ª•c",
    "quy ch·∫ø", "l·ªãch h·ªçc", "l·ªãch thi", "l·ªãch ngh·ªâ", "ho·∫°t ƒë·ªông ngo·∫°i kh√≥a", "c√¢u l·∫°c b·ªô",
]


NEGATE_FOR_CTDT = ["diem chuan", "diem san", "to hop", "ma nganh"]
NEGATE_FOR_DIEM = ["tin chi", "hoc phan", "syllabus", "clo", "plo"]

CTDT_KEYWORDS = {vn_normalize(x) for x in CTDT_KEYWORDS_RAW}
DIEM_KEYWORDS = {vn_normalize(x) for x in DIEM_KEYWORDS_RAW}
HOU_KEYWORDS = {vn_normalize(x) for x in HOU_KEYWORDS_RAW}
NEGATE_FOR_CTDT = {vn_normalize(x) for x in NEGATE_FOR_CTDT}
NEGATE_FOR_DIEM = {vn_normalize(x) for x in NEGATE_FOR_DIEM}


class State(TypedDict):
    query: str
    section: str
    documents: List[Document]
    answer: str
    messages: Annotated[Sequence[BaseMessage], add_messages]  # cho ph√©p auto-append


def last_user_text(messages: Sequence[BaseMessage]) -> str:
    for m in reversed(messages):
        if isinstance(m, HumanMessage):
            return (m.content or "").strip()
    return ""

def year_priority_filter(query: str, docs: List[Document]) -> List[Document]:
    match = YEAR_RE.search(query)
    if not match:
        return docs
    target_year = match.group(1)

    def document_has_year(d: Document) -> bool:
        meta = d.metadata or {}
        combined_text = (d.page_content or "") + " " + " ".join(
            str(meta.get(k, "")) for k in ["date", "title", "url", "section", "category", "year"]
        )
        return target_year in combined_text

    docs_with_year = [d for d in docs if document_has_year(d)]
    docs_without_year = [d for d in docs if not document_has_year(d)]
    return docs_with_year + docs_without_year

def deduplicate_documents(documents: List[Document]) -> List[Document]:
    seen_keys = set()
    unique_documents: List[Document] = []
    for doc in documents:
        key = (
            (doc.metadata or {}).get("source", ""),
            (doc.metadata or {}).get("page", (doc.metadata or {}).get("loc", "")),
            hash(doc.page_content),
        )
        if key in seen_keys:
            continue
        seen_keys.add(key)
        unique_documents.append(doc)
    return unique_documents

# [CHANGE] Router: ∆∞u ti√™n ƒêi·ªÉm ‚Üí CTƒêT ‚Üí HOU; d√πng chu·ªói ƒë√£ chu·∫©n h√≥a kh√¥ng d·∫•u
def decide_scope(query: str, section_value: str) -> str:
    q = vn_normalize(query)

    if any(k in q for k in DIEM_KEYWORDS) and not any(k in q for k in NEGATE_FOR_DIEM):
        return "diem"

    if any(k in q for k in CTDT_KEYWORDS) and not any(k in q for k in NEGATE_FOR_CTDT):
        return "ctdt"

    if (any(k in q for k in HOU_KEYWORDS)) or (section_value in
        ["gi·ªõi thi·ªáu", "tuy·ªÉn sinh", "ng√†nh h·ªçc", "s·ª± ki·ªán", "th√¥ng b√°o", "h·ª£p t√°c", "h·ªçc ph√≠"]):
        return "hou"

    return "both"

def retrieve_with_threshold(query: str, min_score: float = 0.35, k_cap: int = 100) -> List[Document]:
    scored_documents: List[tuple[Document, float]] = []

    for store in selected_vector_stores:
        results = store.similarity_search_with_relevance_scores(query, k=k_cap)
        for doc, score in results:
            numeric_score = score if score is not None else 0.0
            if (score is None) or (numeric_score >= min_score):
                scored_documents.append((doc, numeric_score))

    if not scored_documents:
        for store in selected_vector_stores:
            results = store.similarity_search_with_relevance_scores(query, k=k_cap)
            for doc, score in results:
                scored_documents.append((doc, score if score is not None else 0.0))

    scored_documents.sort(key=lambda item: item[1], reverse=True)
    merged_documents = deduplicate_documents([doc for doc, _ in scored_documents])

    if len(merged_documents) < 4:
        raw_documents: List[Document] = []
        for store in selected_vector_stores:
            raw_documents.extend([doc for doc, _ in store.similarity_search_with_relevance_scores(query, k=k_cap)])
        merged_documents = deduplicate_documents(raw_documents)[:10]

    return merged_documents

def retrieve_with_mmr(query: str, section_value: str) -> List[Document]:
    collected_documents: List[Document] = []

    for store in selected_vector_stores:
        try:
            docs = store.max_marginal_relevance_search(
                query=query,
                k=100,
                fetch_k=100,
                lambda_mult=0.5,
            )
        except Exception:
            docs = []
        collected_documents.extend(docs)

    # [NEW] L·ªçc theo section sau khi l·∫•y MMR (√°p d·ª•ng t·ªët v·ªõi HOU/CTƒêT)
    if section_value and section_value != "kh√°c":
        def match_section(d: Document) -> bool:
            sec = (d.metadata or {}).get("section", "")
            return section_value in sec
        post = [d for d in collected_documents if match_section(d)]
        if post:
            collected_documents = post

    unique_documents = deduplicate_documents(collected_documents)

    if not unique_documents:
        fallback_documents: List[Document] = []
        for store in selected_vector_stores:
            try:
                fallback_documents.extend(
                    store.max_marginal_relevance_search(query=query, k=100, fetch_k=100, lambda_mult=0.5)
                )
            except Exception:
                pass
        unique_documents = deduplicate_documents(fallback_documents)

    return unique_documents

# === Node 1: ph√¢n lo·∫°i + r√∫t ra query t·ª´ messages ===
def classify_section(state: State) -> State:
    query = (state.get("query") or last_user_text(state.get("messages", [])) or "").strip()
    section = section_chain.invoke({"query": query}).content.strip().lower()
    if section not in [
        "gi·ªõi thi·ªáu",
        "tuy·ªÉn sinh",
        "ng√†nh h·ªçc",
        "s·ª± ki·ªán",
        "th√¥ng b√°o",
        "h·ª£p t√°c",
        "h·ªçc ph√≠",
        "kh√°c",
    ]:
        section = "kh√°c"
    print(f"üìé Gemini x√°c ƒë·ªãnh section: {section}")
    return {**state, "section": section}

# === Node 2: retrieve ===
def retrieve_docs(state: State) -> State:
    query = state["query"]
    section_value = state["section"]

    scope = decide_scope(query, section_value)
    global selected_vector_stores
    if scope == "diem":
        selected_vector_stores = [vectordb_diem]
    elif scope == "ctdt":
        selected_vector_stores = [vectordb_ctdt]
    elif scope == "hou":
        selected_vector_stores = [vectordb_hou]
    else:
        # [CHANGE] BOTH: ∆∞u ti√™n HOU + CTƒêT r·ªìi m·ªõi ƒë·∫øn ƒêi·ªÉm
        selected_vector_stores = [vectordb_hou, vectordb_ctdt, vectordb_diem]
    print(f"üîÄ Router scope: {scope} (s·ªë kho: {len(selected_vector_stores)})")

    has_year = bool(YEAR_RE.search(query))
    if scope in ("diem",) or has_year:
        documents = retrieve_with_threshold(query, min_score=0.35, k_cap=100)
        if len(documents) < 4:
            documents = retrieve_with_mmr(query, section_value)
    else:
        documents = retrieve_with_mmr(query, section_value)
        if not documents:
            documents = retrieve_with_threshold(query, min_score=0.35, k_cap=100)

    documents = year_priority_filter(query, documents)
    return {**state, "documents": documents}

# === Node 3: tr·∫£ l·ªùi + ƒë·∫©y AIMessage v√†o b·ªô nh·ªõ ===
def truncate_docs(documents: List[Document], max_chars=4000) -> str:
    combined = ""
    for doc in documents:
        header = []
        meta = doc.metadata or {}
        # [NEW] Nh√∫ng th√™m meta v√†o ng·ªØ c·∫£nh ƒë·ªÉ LLM tr·∫£ l·ªùi ch√≠nh x√°c h∆°n
        for k in ("section", "major", "course_name", "course_code", "credits", "semester", "year"):
            if meta.get(k) not in (None, ""):
                header.append(f"{k}: {meta.get(k)}")
        header_text = ("[" + " | ".join(header) + "]\n") if header else ""
        block = header_text + (doc.page_content or "")
        if len(combined) + len(block) < max_chars:
            combined += block + "\n\n"
        else:
            break
    return combined

def generate_answer(state: State) -> State:
    docs_text = truncate_docs(state["documents"], max_chars=4000)
    if not docs_text.strip():
        content = "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."
        return {**state, "messages": [AIMessage(content=content)], "answer": content}

    try:
        resp = answer_chain.invoke({"context": docs_text, "messages": state["messages"], "question": state["query"]})
        content = (resp.content or "").strip() or "Kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p ho·∫∑c l·ªói t·ª´ Gemini."
        return {**state, "messages": [AIMessage(content=content)], "answer": content}
    except Exception:
        content = "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi do l·ªói API."
        return {**state, "messages": [AIMessage(content=content)], "answer": content}

def add_user_message(state: State) -> State:
    return {**state, "messages": [HumanMessage(content=state["query"])]}

# === X√¢y graph c√≥ checkpointer (MemorySaver) ===
graph = StateGraph(State)
graph.add_node("add_user_message", add_user_message)
graph.add_node("classify", classify_section)
graph.add_node("retrieve", retrieve_docs)
graph.add_node("answer", generate_answer)
graph.set_entry_point("add_user_message")
graph.add_edge("add_user_message", "classify")
graph.add_edge("classify", "retrieve")
graph.add_edge("retrieve", "answer")
graph.set_finish_point("answer")

# B·∫¨T B·ªò NH·ªö: m·ªói thread_id s·∫Ω c√≥ l·ªãch s·ª≠ messages ri√™ng
memory = MemorySaver()
chatbot = graph.compile(checkpointer=memory)

# === CLI test khi ch·∫°y tr·ª±c ti·∫øp ===
if __name__ == "__main__":
    cfg = {"configurable": {"thread_id": "demo"}}
    # In ra path ƒëang d√πng ƒë·ªÉ d·ªÖ debug
    print("[INFO] HOU index:", hou_index_path)
    print("[INFO] DIEM index:", diem_index_path)
    print("[INFO] CTDT index:", ctdt_index_path)
    while True:
        q = input("‚ùì H·ªèi: ").strip()
        if q.lower() in ["exit", "quit", "q"]:
            break
        result = chatbot.invoke({"query": q}, cfg)
        print("\nüìå Tr·∫£ l·ªùi:", result.get("answer"))
