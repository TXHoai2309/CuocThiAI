# chatbot/query_chat.py
import os
import re
from typing import TypedDict, List
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, START
from langchain.chains import LLMChain

# === API Key ===
os.environ["GOOGLE_API_KEY"] = "AIzaSyDR1eVkKtTN3RBeXNdW3bThRIwMMMfJND8"

# === Load FAISS ===
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

base_dir = os.path.dirname(os.path.abspath(__file__))
index_path = os.path.join(base_dir, "..", "hou_crawler", "crawler", "data", "hou_index")

vectordb = FAISS.load_local(
    index_path,
    embeddings=embedding_model,
    index_name="index",
    allow_dangerous_deserialization=True
)

# === Gemini model ===
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=os.environ["GOOGLE_API_KEY"],
    temperature=0.5,
    max_output_tokens=2048
)

# === Prompt tr·∫£ l·ªùi ch√≠nh ===
chat_main = ChatPromptTemplate.from_messages([
    ("system",
     "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c tinh ch·ªânh. "
     "∆Øu ti√™n ch√≠nh x√°c, logic, d·ªÖ hi·ªÉu, v√† ch·ªâ d√πng th√¥ng tin trong ph·∫ßn 'T√†i li·ªáu'."),
    # ƒê∆∞a context v√†o d∆∞·ªõi vai tr√≤ system ƒë·ªÉ m√¥ h√¨nh xem ƒë√¢y l√† ngu·ªìn s·ª± th·∫≠t
    ("system", "T√†i li·ªáu (context):\n{context}"),
    ("user", "{question}")
])
answer_chain = chat_main | llm  # pipe: prompt -> llm

# === Prompt ph√¢n lo·∫°i section ===
chat_section = ChatPromptTemplate.from_messages([
    ("system",
     "B·∫°n l√† b·ªô ph√¢n lo·∫°i √Ω ƒë·ªãnh. H√£y tr·∫£ v·ªÅ ƒë√∫ng 1 t·ª´ trong: "
     "'gi·ªõi thi·ªáu', 'tuy·ªÉn sinh', 'ng√†nh h·ªçc', 's·ª± ki·ªán', 'th√¥ng b√°o', 'h·ª£p t√°c', 'h·ªçc ph√≠', 'kh√°c'. "
     "Kh√¥ng th√™m k√Ω t·ª± th·ª´a."),
    ("user", "C√¢u h·ªèi: {query}\nPh√¢n lo·∫°i:")
])
section_chain = chat_section | llm  # pipe: prompt -> llm

# === Khai b√°o tr·∫°ng th√°i LangGraph ===
class QAState(TypedDict):
    query: str
    section: str
    documents: List[Document]
    answer: str


# === Node 1: Ph√¢n lo·∫°i section ===
def classify_section(state: QAState) -> QAState:
    section = section_chain.invoke({"query": state["query"]}).content.strip().lower()
    if section not in ["gi·ªõi thi·ªáu", "tuy·ªÉn sinh", "ng√†nh h·ªçc", "s·ª± ki·ªán", "th√¥ng b√°o", "h·ª£p t√°c", "h·ªçc ph√≠", "kh√°c"]:
        section = "kh√°c"
    print(f"üìé Gemini x√°c ƒë·ªãnh section: {section}")
    return {**state, "section": section}


# === Node 2: Truy xu·∫•t t√†i li·ªáu t·ª´ FAISS ===
def retrieve_docs(state: QAState) -> QAState:
    # Th·ª≠ t√¨m theo section n·∫øu c√≥ metadata
    try:
        docs = vectordb.max_marginal_relevance_search(
            query=state["query"],
            k=100,
            fetch_k=100,
            lambda_mult=0.5,
            filter=lambda meta: state["section"] in meta.get("section", "")
        )
    except:
        docs = []  # n·∫øu FAISS kh√¥ng h·ªó tr·ª£ filter, fallback

    # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£, fallback: kh√¥ng d√πng filter
    if not docs:
        docs = vectordb.max_marginal_relevance_search(
            query=state["query"],
            k=100,
            fetch_k=100,
            lambda_mult=0.5
        )
        print("‚ö†Ô∏è Kh√¥ng t√¨m ƒë∆∞·ª£c t√†i li·ªáu theo section, d√πng to√†n b·ªô FAISS.")

    return {**state, "documents": docs}



# === Node 3: T·∫°o c√¢u tr·∫£ l·ªùi t·ª´ context ===
def truncate_docs(documents, max_chars=6000):
    combined = ""
    for doc in documents:
        if len(combined) + len(doc.page_content) < max_chars:
            combined += doc.page_content + "\n\n"
        else:
            break
    return combined

def generate_answer(state: QAState) -> QAState:
    docs_text = truncate_docs(state["documents"], max_chars=4000)  # Gi·ªØ ng∆∞·ª°ng 4k k√Ω t·ª± ƒë·ªÉ gi·∫£m r·ªßi ro tr√†n token
    if not docs_text.strip():
        print("‚ö†Ô∏è Empty context detected!")
        return {**state, "answer": "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."}

    # CHANGED: g·ªçi answer_chain.invoke v·ªõi {context, question}, thay v√¨ format PromptTemplate r·ªìi llm.invoke chu·ªói.
    try:
        resp = answer_chain.invoke({"context": docs_text, "question": state["query"]})
        content = (resp.content or "").strip()
        if not content:
            return {**state, "answer": "Kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p ho·∫∑c l·ªói t·ª´ Gemini."}
        return {**state, "answer": content}
    except Exception as e:
        print("‚ùå Gemini error:", str(e))
        return {**state, "answer": "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi do l·ªói API."}



# === X√¢y LangGraph ===
graph = StateGraph(QAState)
graph.add_node("classify", classify_section)
graph.add_node("retrieve", retrieve_docs)
graph.add_node("answer", generate_answer)

graph.set_entry_point("classify")
graph.add_edge("classify", "retrieve")
graph.add_edge("retrieve", "answer")
graph.set_finish_point("answer")

# ... c√°c ph·∫ßn tr√™n gi·ªØ nguy√™n ...

chatbot = graph.compile()

# === V√≤ng l·∫∑p CLI ch·ªâ ch·∫°y khi ch·∫°y tr·ª±c ti·∫øp file n√†y ===
if __name__ == "__main__":
    while True:
        query = input("‚ùì H·ªèi: ").strip()
        if query.lower() in ["exit", "quit", "q"]:
            print("üëã T·∫°m bi·ªát!")
            break
        result = chatbot.invoke({"query": query})
        print("\nüìå Tr·∫£ l·ªùi:", result["answer"])
