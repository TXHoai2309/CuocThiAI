# chatbot/query_chat.py
import os
import re
from typing import Annotated, TypedDict, List, Sequence

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

# === API Key ===
os.environ.setdefault("GOOGLE_API_KEY", "AIzaSyDR1eVkKtTN3RBeXNdW3bThRIwMMMfJND8")

# === Load FAISS ===
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
base_dir = os.path.dirname(os.path.abspath(__file__))

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn index FAISS
hou_index_path = os.path.join(base_dir, "..", "data", "hou", "hou_index")
diem_index_path = os.path.join(base_dir, "..", "data", "hou", "diem_chuan_index")

# T·∫£i index FAISS v·ªõi embeddings
vectordb_hou = FAISS.load_local(
    hou_index_path,
    embeddings=embedding_model,
    index_name="index",
    allow_dangerous_deserialization=True,
)

# [GI·ªÆ NGUY√äN THEO CODE C·ª¶A B·∫†N]
# L∆∞u √Ω: index_name ("diem_index") ph·∫£i TR√ôNG v·ªõi l√∫c build FAISS cho kho ƒëi·ªÉm.
vectordb_diem = FAISS.load_local(
    diem_index_path,
    embeddings=embedding_model,
    index_name="index",
    allow_dangerous_deserialization=True,
)

# [S·ª¨A] Danh s√°ch c√°c kho ƒë∆∞·ª£c d√πng cho l·∫ßn truy v·∫•n hi·ªán t·∫°i (router s·∫Ω c·∫≠p nh·∫≠t)
# L√Ω do: Cho ph√©p ch·ªçn HOU/ƒêi·ªÉm/c·∫£ hai m√† KH√îNG ƒë·ªïi t√™n h√†m retrieve g·ªëc.
selected_vector_stores: List[FAISS] = [vectordb_hou]

# === Gemini model ===
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    google_api_key=os.environ["GOOGLE_API_KEY"],
    temperature=0.5,
    max_output_tokens=2048,
)

# === Prompt tr·∫£ l·ªùi ch√≠nh (RAG) ===
chat_main = ChatPromptTemplate.from_messages([
    (
        "system",
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c tinh ch·ªânh. "
        "∆Øu ti√™n ch√≠nh x√°c, logic, d·ªÖ hi·ªÉu, v√† ch·ªâ d√πng th√¥ng tin trong ph·∫ßn 'T√†i li·ªáu'."
        "v·ªõi ng·ªØ c·∫£nh h·ªôi tho·∫°i (v√≠ d·ª•: t√™n ng∆∞·ªùi d√πng, tham chi·∫øu nh∆∞ 'ng√†nh n√†y'), b·∫°n ƒë∆∞·ª£c ph√©p d√πng l·ªãch s·ª≠ h·ªôi tho·∫°i"
        "H√£y TR√åNH B√ÄY K·∫æT QU·∫¢ THEO MARKDOWN, ng·∫Øn g·ªçn v√† r√µ r√†ng, ∆∞u ti√™n:\n"
        "1) Ti√™u ƒë·ªÅ c·∫•p 3 tr·ªü xu·ªëng (###),\n"
        "2) Danh s√°ch g·∫°ch ƒë·∫ßu d√≤ng cho c√°c √Ω ch√≠nh,\n"
        "3) B·∫£ng khi so s√°nh/ li·ªát k√™ theo c·ªôt,\n"
        "4) Chia ƒëo·∫°n ng·∫Øn; kh√¥ng vi·∫øt th√†nh m·ªôt kh·ªëi d√†i.\n"
        "∆Øu ti√™n ch√≠nh x√°c, logic, d·ªÖ hi·ªÉu lu√¥n ho√†n th√†nh c√¢u tr·ªçn v·∫πn.",
    ),
    MessagesPlaceholder(variable_name="messages"),
    ("user",
     "D·ª±a tr√™n T√ÄI LI·ªÜU d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi.\n\n"
     "T√ÄI LI·ªÜU:\n{context}\n\n"
     "C√ÇU H·ªéI: {question}\n\nTr·∫£ l·ªùi:")
])
answer_chain = chat_main | llm

# === Prompt ph√¢n lo·∫°i section ===
chat_section = ChatPromptTemplate.from_messages([
    (
        "system",
        "B·∫°n l√† b·ªô ph√¢n lo·∫°i √Ω ƒë·ªãnh. H√£y tr·∫£ v·ªÅ ƒë√∫ng 1 t·ª´ trong: "
        "'gi·ªõi thi·ªáu', 'tuy·ªÉn sinh', 'ng√†nh h·ªçc', 's·ª± ki·ªán', 'th√¥ng b√°o', 'h·ª£p t√°c', 'h·ªçc ph√≠', 'kh√°c'. "
        "Kh√¥ng th√™m k√Ω t·ª± th·ª´a.",
    ),
    ("user", "C√¢u h·ªèi: {query}\nPh√¢n lo·∫°i:"),
])
section_chain = chat_section | llm

# === Heuristics: regex nƒÉm ƒë·ªÉ ∆∞u ti√™n t√†i li·ªáu ch·ª©a nƒÉm ===
YEAR_RE = re.compile(r"(20\d{2})")

# --- Router keywords ---
DIEM_KEYWORDS = ["ƒëi·ªÉm", "ƒëi·ªÉm chu·∫©n", "ƒëi·ªÉm s√†n", "ng∆∞·ª°ng", "t·ªï h·ª£p", "m√£ ng√†nh", "ch·ªâ ti√™u"]
HOU_KEYWORDS = [
    "ng√†nh", "ch∆∞∆°ng tr√¨nh", "h·ªçc ph√≠", "th√¥ng b√°o", "s·ª± ki·ªán", "tuy·ªÉn sinh",
    "gi·ªõi thi·ªáu", "h·ª£p t√°c", "khoa", "vi·ªán", "m√¥n", "h·ªçc ph·∫ßn", "h·ªì s∆°"
]

# === Khai b√°o STATE c√≥ b·ªô nh·ªõ tin nh·∫Øn ===
class State(TypedDict):
    query: str
    section: str
    documents: List[Document]
    answer: str
    messages: Annotated[Sequence[BaseMessage], add_messages]  # cho ph√©p auto-append



def last_user_text(messages: Sequence[BaseMessage]) -> str:
    for m in reversed(messages):
        if isinstance(m, HumanMessage):
            return (m.content or "").strip()
    return ""


# [S·ª¨A] B·ªé `FACT_KEYWORDS` v√† `is_fact_query`.
# L√Ω do: Router theo DIEM/HOU ƒë√£ ƒë·ªß. Ta d√πng (1) scope v√† (2) c√≥-nƒÉm hay kh√¥ng ƒë·ªÉ ch·ªçn chi·∫øn l∆∞·ª£c retrieve.


def year_priority_filter(query: str, docs: List[Document]) -> List[Document]:
    match = YEAR_RE.search(query)
    if not match:
        return docs
    target_year = match.group(1)

    def document_has_year(d: Document) -> bool:
        meta = d.metadata or {}
        combined_text = (d.page_content or "") + " " + " ".join(
            str(meta.get(k, "")) for k in ["date", "title", "url", "section", "category", "year"]
        )
        return target_year in combined_text

    docs_with_year = [d for d in docs if document_has_year(d)]
    docs_without_year = [d for d in docs if not document_has_year(d)]
    return docs_with_year + docs_without_year


# [S·ª¨A] Kh·ª≠ tr√πng l·∫∑p khi g·ªôp k·∫øt qu·∫£ t·ª´ nhi·ªÅu kho
def deduplicate_documents(documents: List[Document]) -> List[Document]:
    seen_keys = set()
    unique_documents: List[Document] = []
    for doc in documents:
        key = (
            doc.metadata.get("source", ""),
            doc.metadata.get("page", doc.metadata.get("loc", "")),
            hash(doc.page_content),
        )
        if key in seen_keys:
            continue
        seen_keys.add(key)
        unique_documents.append(doc)
    return unique_documents


# [S·ª¨A] Quy·∫øt ƒë·ªãnh ph·∫°m vi t√¨m ki·∫øm d·ª±a tr√™n c√¢u h·ªèi v√† section
def decide_scope(query: str, section_value: str) -> str:
    query_lower = query.lower()
    if any(k in query_lower for k in DIEM_KEYWORDS):
        return "diem"
    if (any(k in query_lower for k in HOU_KEYWORDS)) or (section_value in
        ["gi·ªõi thi·ªáu", "tuy·ªÉn sinh", "ng√†nh h·ªçc", "s·ª± ki·ªán", "th√¥ng b√°o", "h·ª£p t√°c", "h·ªçc ph√≠"]):
        return "hou"
    return "both"


def retrieve_with_threshold(query: str, min_score: float = 0.35, k_cap: int = 100) -> List[Document]:
    # [S·ª¨A] Duy·ªát tr√™n "selected_vector_stores" (HOU/ƒêi·ªÉm/c·∫£ hai) thay v√¨ ch·ªâ HOU
    scored_documents: List[tuple[Document, float]] = []

    for store in selected_vector_stores:
        results = store.similarity_search_with_relevance_scores(query, k=k_cap)
        for doc, score in results:
            numeric_score = score if score is not None else 0.0
            if (score is None) or (numeric_score >= min_score):
                scored_documents.append((doc, numeric_score))

    # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ v∆∞·ª£t ng∆∞·ª°ng, n·ªõi l·ªèng: l·∫•y top-k th√¥
    if not scored_documents:
        for store in selected_vector_stores:
            results = store.similarity_search_with_relevance_scores(query, k=k_cap)
            for doc, score in results:
                scored_documents.append((doc, score if score is not None else 0.0))

    scored_documents.sort(key=lambda item: item[1], reverse=True)
    merged_documents = deduplicate_documents([doc for doc, _ in scored_documents])

    # N·∫øu sau l·ªçc c√≤n qu√° √≠t ‚Üí tr·∫£ th√™m m·ªôt √≠t top (t·ªëi ƒëa 10)
    if len(merged_documents) < 4:
        raw_documents: List[Document] = []
        for store in selected_vector_stores:
            raw_documents.extend([doc for doc, _ in store.similarity_search_with_relevance_scores(query, k=k_cap)])
        merged_documents = deduplicate_documents(raw_documents)[:10]

    return merged_documents


def retrieve_with_mmr(query: str, section_value: str) -> List[Document]:
    # [S·ª¨A] Ch·∫°y MMR tr√™n c√°c kho ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh tuy·∫øn
    # Kho HOU c√≥ 'section' ·ªïn ƒë·ªãnh ‚Üí √°p filter theo section ƒë·ªÉ tƒÉng ph√π h·ª£p
    collected_documents: List[Document] = []

    for store in selected_vector_stores:
        try:
            if store is vectordb_hou:
                docs = store.max_marginal_relevance_search(
                    query=query,
                    k=100,
                    fetch_k=100,
                    lambda_mult=0.5,
                    filter=lambda meta: section_value in meta.get("section", ""),
                )
            else:
                docs = store.max_marginal_relevance_search(
                    query=query,
                    k=100,
                    fetch_k=100,
                    lambda_mult=0.5,
                )
        except Exception:
            docs = []
        collected_documents.extend(docs)

    unique_documents = deduplicate_documents(collected_documents)

    # Fallback MMR kh√¥ng filter n·∫øu v·∫´n thi·∫øu t√†i li·ªáu
    if not unique_documents:
        fallback_documents: List[Document] = []
        for store in selected_vector_stores:
            try:
                fallback_documents.extend(
                    store.max_marginal_relevance_search(query=query, k=100, fetch_k=100, lambda_mult=0.5)
                )
            except Exception:
                pass
        unique_documents = deduplicate_documents(fallback_documents)

    return unique_documents

# === Node 1: ph√¢n lo·∫°i + r√∫t ra query t·ª´ messages ===

def classify_section(state: State) -> State:
    query = (state.get("query") or last_user_text(state.get("messages", [])) or "").strip()
    section = section_chain.invoke({"query": query}).content.strip().lower()
    if section not in [
        "gi·ªõi thi·ªáu",
        "tuy·ªÉn sinh",
        "ng√†nh h·ªçc",
        "s·ª± ki·ªán",
        "th√¥ng b√°o",
        "h·ª£p t√°c",
        "h·ªçc ph√≠",
        "kh√°c",
    ]:
        section = "kh√°c"
    print(f"üìé Gemini x√°c ƒë·ªãnh section: {section}")
    return {**state, "section": section}

# === Node 2: retrieve ===

def retrieve_docs(state: State) -> State:
    query = state["query"]
    section_value = state["section"]

    # [S·ª¨A] ƒê·ªãnh tuy·∫øn tr∆∞·ªõc: ch·ªâ ƒêi·ªÉm / ch·ªâ HOU / c·∫£ hai
    scope = decide_scope(query, section_value)
    global selected_vector_stores
    if scope == "diem":
        selected_vector_stores = [vectordb_diem]
    elif scope == "hou":
        selected_vector_stores = [vectordb_hou]
    else:
        selected_vector_stores = [vectordb_hou, vectordb_diem]
    print(f"üîÄ Router scope: {scope} (s·ªë kho: {len(selected_vector_stores)})")

    # [S·ª¨A] B·ªé `is_fact_query`: thay b·∫±ng chi·∫øn l∆∞·ª£c d·ª±a tr√™n scope v√† nƒÉm
    # - N·∫øu scope == 'diem' ho·∫∑c c√¢u h·ªèi c√≥ nƒÉm ‚Üí threshold tr∆∞·ªõc, thi·∫øu th√¨ MMR
    # - Ng∆∞·ª£c l·∫°i ‚Üí MMR tr∆∞·ªõc, thi·∫øu th√¨ threshold
    has_year = bool(YEAR_RE.search(query))
    if scope == "diem" or has_year:
        documents = retrieve_with_threshold(query, min_score=0.35, k_cap=100)
        if len(documents) < 4:
            documents = retrieve_with_mmr(query, section_value)
    else:
        documents = retrieve_with_mmr(query, section_value)
        if not documents:
            documents = retrieve_with_threshold(query, min_score=0.35, k_cap=100)

    # ∆Øu ti√™n theo nƒÉm (n·∫øu c√≥ nƒÉm trong c√¢u h·ªèi)
    documents = year_priority_filter(query, documents)
    return {**state, "documents": documents}

# === Node 3: tr·∫£ l·ªùi + ƒë·∫©y AIMessage v√†o b·ªô nh·ªõ ===

def truncate_docs(documents: List[Document], max_chars=4000) -> str:
    combined = ""
    for doc in documents:
        if len(combined) + len(doc.page_content) < max_chars:
            combined += doc.page_content + "\n\n"
        else:
            break
    return combined


def generate_answer(state: State) -> State:
    docs_text = truncate_docs(state["documents"], max_chars=4000)
    if not docs_text.strip():
        content = "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."
        return {**state, "messages": [AIMessage(content=content)], "answer": content}

    try:
        # Truy·ªÅn c·∫£ l·ªãch s·ª≠ h·ªôi tho·∫°i ƒë·ªÉ h·ªó tr·ª£ tham chi·∫øu ng·ªØ c·∫£nh khi c·∫ßn
        resp = answer_chain.invoke({"context": docs_text, "messages": state["messages"], "question": state["query"]})
        content = (resp.content or "").strip() or "Kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p ho·∫∑c l·ªói t·ª´ Gemini."
        return {**state, "messages": [AIMessage(content=content)], "answer": content}
    except Exception:
        content = "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi do l·ªói API."
        return {**state, "messages": [AIMessage(content=content)], "answer": content}



def add_user_message(state: State) -> State:
    return {**state, "messages": [HumanMessage(content=state["query"])]}

# === X√¢y graph c√≥ checkpointer (MemorySaver) ===
graph = StateGraph(State)

graph.add_node("add_user_message", add_user_message)

graph.add_node("classify", classify_section)
graph.add_node("retrieve", retrieve_docs)
graph.add_node("answer", generate_answer)

graph.set_entry_point("add_user_message")
graph.add_edge("add_user_message", "classify")
graph.add_edge("classify", "retrieve")
graph.add_edge("retrieve", "answer")
graph.set_finish_point("answer")

# B·∫¨T B·ªò NH·ªö: m·ªói thread_id s·∫Ω c√≥ l·ªãch s·ª≠ messages ri√™ng
memory = MemorySaver()
chatbot = graph.compile(checkpointer=memory)

# === CLI test khi ch·∫°y tr·ª±c ti·∫øp ===
if __name__ == "__main__":
    cfg = {"configurable": {"thread_id": "demo"}}
    while True:
        q = input("‚ùì H·ªèi: ").strip()
        if q.lower() in ["exit", "quit", "q"]:
            break
        result = chatbot.invoke({"query": q}, cfg)
        print("\nüìå Tr·∫£ l·ªùi:", result.get("answer"))
