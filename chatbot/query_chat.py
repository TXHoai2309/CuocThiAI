import os
from typing import TypedDict, List
from click import prompt
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langgraph.graph import StateGraph, START
from langchain.chains import LLMChain

# === API Key ===
os.environ["GOOGLE_API_KEY"] = "AIzaSyDR1eVkKtTN3RBeXNdW3bThRIwMMMfJND8"

# === Load FAISS ===
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectordb = FAISS.load_local(
    r"D:\airdrop\CuocThiAI\hou_crawler\crawler\data\hou_index",
    embeddings=embedding_model,
    index_name="index",
    allow_dangerous_deserialization=True
)

# === Gemini model ===
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=os.environ["GOOGLE_API_KEY"],
    temperature=0.5,
    max_output_tokens=2048
)

# === Prompt tráº£ lá»i chÃ­nh ===
main_prompt = PromptTemplate.from_template("""Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh, tráº£ lá»i cÃ¢u há»i dá»±a trÃªn dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tinh chá»‰nh. HÃ£y tá»•ng há»£p vÃ  cung cáº¥p cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c, logic, dá»… hiá»ƒu vÃ  sÃ¡ng táº¡o náº¿u cáº§n dá»±a trÃªn thÃ´ng tin tá»« tÃ i liá»‡u sau:

**TÃ i liá»‡u**: {context}

**CÃ¢u há»i**: {question}

**Tráº£ lá»i**:""")

# === Prompt phÃ¢n loáº¡i section ===
section_prompt = PromptTemplate.from_template(
    """PhÃ¢n tÃ­ch cÃ¢u há»i sau vÃ  tráº£ lá»i báº±ng Ä‘Ãºng 1 tá»« trong: 
'giá»›i thiá»‡u', 'tuyá»ƒn sinh', 'ngÃ nh há»c', 'sá»± kiá»‡n', 'thÃ´ng bÃ¡o', 'há»£p tÃ¡c', 'há»c phÃ­', 'khÃ¡c'.\n\nCÃ¢u há»i: {query}\nPhÃ¢n loáº¡i:"""
)
section_chain = section_prompt | llm

# === Khai bÃ¡o tráº¡ng thÃ¡i LangGraph ===
class QAState(TypedDict):
    query: str
    section: str
    documents: List[Document]
    answer: str

# === Node 1: PhÃ¢n loáº¡i section ===
def classify_section(state: QAState) -> QAState:
    section = section_chain.invoke({"query": state["query"]}).content.strip().lower()
    if section not in ["giá»›i thiá»‡u", "tuyá»ƒn sinh", "ngÃ nh há»c", "sá»± kiá»‡n", "thÃ´ng bÃ¡o", "há»£p tÃ¡c", "há»c phÃ­", "khÃ¡c"]:
        section = "khÃ¡c"
    print(f"ğŸ“ Gemini xÃ¡c Ä‘á»‹nh section: {section}")
    return {**state, "section": section}


# === Node 2: Truy xuáº¥t tÃ i liá»‡u tá»« FAISS ===
def retrieve_docs(state: QAState) -> QAState:
    # Thá»­ tÃ¬m theo section náº¿u cÃ³ metadata
    try:
        docs = vectordb.max_marginal_relevance_search(
            query=state["query"],
            k=30,
            fetch_k=100,
            lambda_mult=0.5,
            filter=lambda meta: state["section"] in meta.get("section", "")
        )
    except:
        docs = []  # náº¿u FAISS khÃ´ng há»— trá»£ filter, fallback

    # Náº¿u khÃ´ng cÃ³ káº¿t quáº£, fallback: khÃ´ng dÃ¹ng filter
    if not docs:
        docs = vectordb.max_marginal_relevance_search(
            query=state["query"],
            k=30,
            fetch_k=100,
            lambda_mult=0.5
        )
        print("âš ï¸ KhÃ´ng tÃ¬m Ä‘Æ°á»£c tÃ i liá»‡u theo section, dÃ¹ng toÃ n bá»™ FAISS.")

    return {**state, "documents": docs}



# === Node 3: Táº¡o cÃ¢u tráº£ lá»i tá»« context ===
def truncate_docs(documents, max_chars=6000):
    combined = ""
    for doc in documents:
        if len(combined) + len(doc.page_content) < max_chars:
            combined += doc.page_content + "\n\n"
        else:
            break
    return combined

def generate_answer(state: QAState) -> QAState:
    docs_text = truncate_docs(state["documents"], max_chars=4000)  # Giáº£m max_chars
    print("ğŸ“‘ Context length:", len(docs_text))
    print("ğŸ“‘ Context sample:", docs_text[:500])
    if not docs_text.strip():
        print("âš ï¸ Empty context detected!")
        return {**state, "answer": "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan."}
    
    prompt = main_prompt.format(context=docs_text, question=state["query"])
    print("ğŸ“ Prompt sent to Gemini:", prompt[:500])
    
    try:
        response = llm.invoke(prompt)
        print("ğŸ“¤ Gemini raw response:", response)
        print("ğŸ“¤ Gemini response content:", response.content)
        if not response.content.strip():
            print("âš ï¸ Gemini returned empty response!")
            return {**state, "answer": "KhÃ´ng cÃ³ thÃ´ng tin phÃ¹ há»£p hoáº·c lá»—i tá»« Gemini."}
    except Exception as e:
        print("âŒ Gemini error:", str(e))
        return {**state, "answer": "KhÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i do lá»—i API."}
    
    return {**state, "answer": response.content}



# === XÃ¢y LangGraph ===
graph = StateGraph(QAState)
graph.add_node("classify", classify_section)
graph.add_node("retrieve", retrieve_docs)
graph.add_node("answer", generate_answer)

graph.set_entry_point("classify")
graph.add_edge("classify", "retrieve")
graph.add_edge("retrieve", "answer")
graph.set_finish_point("answer")

chatbot = graph.compile()

# === VÃ²ng láº·p CLI ===
while True:
    query = input("â“ Há»i: ").strip()
    if query.lower() in ["exit", "quit", "q"]:
        print("ğŸ‘‹ Táº¡m biá»‡t!")
        break

    result = chatbot.invoke({"query": query})
    print("\nğŸ“Œ Tráº£ lá»i:", result["answer"])
    print("\nğŸ“š Tá»« cÃ¡c Ä‘oáº¡n:")
    for doc in result["documents"]:
        print(f"- {doc.metadata.get('title', 'KhÃ´ng tiÃªu Ä‘á»')} | {doc.page_content[:100]}...")
